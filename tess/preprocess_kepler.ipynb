{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6f8a22a",
   "metadata": {},
   "source": [
    "# Kepler Exoplanet Data Preprocessing Pipeline - 3-Class Classification\n",
    "\n",
    "This notebook implements a comprehensive preprocessing pipeline for the Kepler exoplanet dataset from NASA's Kepler mission for 3-class classification.\n",
    "\n",
    "## Dataset Information:\n",
    "- **Source**: Kepler Objects of Interest (KOI) Cumulative Table\n",
    "- **Size**: 9,564 objects × 141 features\n",
    "- **Target Classes**: CONFIRMED (2,746), FALSE POSITIVE (4,839), CANDIDATE (1,979)\n",
    "\n",
    "## Preprocessing Steps:\n",
    "1. **Data Loading and Initial Exploration** - Load CSV, check dimensions, data types, and missing values\n",
    "2. **Target Variable Creation** - 3-class classification: Confirmed vs Candidate vs False Positive\n",
    "3. **Feature Selection and Engineering** - Process Kepler-specific features\n",
    "4. **Missing Value Handling** - Clean and impute missing data\n",
    "5. **Data Scaling and Splitting** - Prepare for model training\n",
    "6. **Export Processed Data** - Save for model training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a88d0fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Libraries imported successfully!\n",
      "🚀 Ready for Kepler data preprocessing!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options for better output formatting\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"📚 Libraries imported successfully!\")\n",
    "print(\"🚀 Ready for Kepler data preprocessing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a665f16d",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration\n",
    "\n",
    "Load the Kepler CSV file and perform initial data exploration including:\n",
    "- Dataset dimensions (rows/columns)\n",
    "- Data types examination\n",
    "- Missing value analysis\n",
    "- Target distribution analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a126d70d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'kepler.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load the Kepler dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mkepler.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Basic dataset information\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== KEPLER DATASET OVERVIEW ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'kepler.csv'"
     ]
    }
   ],
   "source": [
    "# Load the Kepler dataset\n",
    "df = pd.read_csv('kepler.csv')\n",
    "\n",
    "# Basic dataset information\n",
    "print(\"=== KEPLER DATASET OVERVIEW ===\")\n",
    "print(f\"📊 Dataset shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "print(f\"💾 Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Target variable analysis\n",
    "print(\"\\n=== TARGET DISTRIBUTION ===\")\n",
    "target_counts = df['koi_disposition'].value_counts()\n",
    "print(target_counts)\n",
    "print(f\"\\n📈 Target percentages:\")\n",
    "for disposition, count in target_counts.items():\n",
    "    pct = count / len(df) * 100\n",
    "    print(f\"   {disposition}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Data types overview\n",
    "print(\"\\n=== DATA TYPES SUMMARY ===\")\n",
    "dtype_counts = df.dtypes.value_counts()\n",
    "print(dtype_counts)\n",
    "\n",
    "# Missing values analysis\n",
    "print(\"\\n=== MISSING VALUES ANALYSIS ===\")\n",
    "missing_total = df.isnull().sum().sum()\n",
    "missing_pct = (missing_total / (df.shape[0] * df.shape[1])) * 100\n",
    "print(f\"Total missing values: {missing_total:,} ({missing_pct:.2f}%)\")\n",
    "\n",
    "# Columns with highest missing values\n",
    "missing_by_col = df.isnull().sum().sort_values(ascending=False)\n",
    "top_missing = missing_by_col[missing_by_col > 0].head(10)\n",
    "if not top_missing.empty:\n",
    "    print(\"\\n📉 Top 10 columns with missing values:\")\n",
    "    for col, missing_count in top_missing.items():\n",
    "        missing_pct_col = (missing_count / len(df)) * 100\n",
    "        print(f\"   {col}: {missing_count} ({missing_pct_col:.1f}%)\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\n=== SAMPLE DATA (First 3 rows) ===\")\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9696c801",
   "metadata": {},
   "source": [
    "## 2. Target Variable Creation - 3-Class Classification\n",
    "\n",
    "Convert the Kepler disposition categories into a 3-class classification problem:\n",
    "- **Candidate (0)**: Objects marked as CANDIDATE\n",
    "- **Confirmed (1)**: Objects marked as CONFIRMED  \n",
    "- **False_Positive (2)**: Objects marked as FALSE POSITIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ec70ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Creating 3-class target variable...\n",
      "\n",
      "📊 FINAL 3-CLASS TARGET DISTRIBUTION:\n",
      "   0 (Candidate): 1979 (20.7%)\n",
      "   1 (Confirmed): 2746 (28.7%)\n",
      "   2 (False_Positive): 4839 (50.6%)\n",
      "\n",
      "⚖️ Class imbalance ratio: 2.4:1\n",
      "\n",
      "✅ Target variable created successfully!\n",
      "📈 Final dataset shape: (9564, 142)\n"
     ]
    }
   ],
   "source": [
    "# Create 3-class target variable\n",
    "print(\"🎯 Creating 3-class target variable...\")\n",
    "\n",
    "# Map Kepler dispositions to 3 classes\n",
    "disposition_mapping = {\n",
    "    'CANDIDATE': 0,        # Planet candidates needing follow-up\n",
    "    'CONFIRMED': 1,        # Confirmed exoplanets\n",
    "    'FALSE POSITIVE': 2    # False positives and refuted objects\n",
    "}\n",
    "\n",
    "# Create target variable\n",
    "df['target_3class'] = df['koi_disposition'].map(disposition_mapping)\n",
    "\n",
    "# Check for any unmapped values\n",
    "unmapped = df[df['target_3class'].isnull()]['koi_disposition'].unique()\n",
    "if len(unmapped) > 0:\n",
    "    print(f\"⚠️ Unmapped dispositions found: {unmapped}\")\n",
    "    # Handle any edge cases here if needed\n",
    "    \n",
    "# Remove rows with missing target\n",
    "initial_count = len(df)\n",
    "df = df.dropna(subset=['target_3class'])\n",
    "removed_count = initial_count - len(df)\n",
    "if removed_count > 0:\n",
    "    print(f\"🗑️ Removed {removed_count} rows with missing target\")\n",
    "\n",
    "# Final target distribution\n",
    "print(\"\\n📊 FINAL 3-CLASS TARGET DISTRIBUTION:\")\n",
    "target_counts = df['target_3class'].value_counts().sort_index()\n",
    "class_names = ['Candidate', 'Confirmed', 'False_Positive']\n",
    "\n",
    "for class_id, count in target_counts.items():\n",
    "    class_name = class_names[int(class_id)]\n",
    "    pct = count / len(df) * 100\n",
    "    print(f\"   {class_id} ({class_name}): {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Class imbalance analysis\n",
    "class_counts = target_counts.values\n",
    "imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "print(f\"\\n⚖️ Class imbalance ratio: {imbalance_ratio:.1f}:1\")\n",
    "\n",
    "# Create target mapping for reference\n",
    "target_mapping = {\n",
    "    'original_mapping': disposition_mapping,\n",
    "    'encoding': {str(k): v for v, k in enumerate(class_names)},\n",
    "    'class_descriptions': {\n",
    "        'Candidate': 'Planet candidates requiring follow-up observation',\n",
    "        'Confirmed': 'Confirmed exoplanets with high confidence',\n",
    "        'False_Positive': 'False positives and refuted planetary candidates'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\n✅ Target variable created successfully!\")\n",
    "print(f\"📈 Final dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628af8ae",
   "metadata": {},
   "source": [
    "## 3. Feature Selection and Engineering\n",
    "\n",
    "Select and engineer relevant features from the Kepler dataset:\n",
    "- Remove non-predictive columns (IDs, names, comments)\n",
    "- Process numerical features\n",
    "- Handle error columns and flags\n",
    "- Create derived features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df852f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Starting feature selection and engineering...\n",
      "📋 Total columns in dataset: 142\n",
      "🎯 Feature columns selected: 126\n",
      "\n",
      "🗑️ Excluded columns (16):\n",
      "   • rowid\n",
      "   • kepid\n",
      "   • kepoi_name\n",
      "   • kepler_name\n",
      "   • koi_disposition\n",
      "   • koi_pdisposition\n",
      "   • target_3class\n",
      "   • koi_comment\n",
      "   • koi_disp_prov\n",
      "   • koi_parm_prov\n",
      "   • koi_sparprov\n",
      "   • koi_tce_delivname\n",
      "   • koi_datalink_dvr\n",
      "   • koi_datalink_dvs\n",
      "   • koi_limbdark_mod\n",
      "   • koi_vet_date\n",
      "\n",
      "📊 FEATURE ANALYSIS:\n",
      "   Numerical features: 122\n",
      "   Object features: 4\n",
      "\n",
      "🔤 Object columns requiring attention:\n",
      "   • koi_vet_stat: 1 unique values\n",
      "     Sample: ['Done']\n",
      "   • koi_fittype: 4 unique values\n",
      "     Sample: ['LS+MCMC' 'MCMC' 'LS' 'none']\n",
      "   • koi_quarters: 212 unique values\n",
      "   • koi_trans_mod: 1 unique values\n",
      "     Sample: ['Mandel and Agol (2002 ApJ 580 171)']\n",
      "\n",
      "✅ Feature selection completed!\n",
      "📈 Features shape: (9564, 126)\n"
     ]
    }
   ],
   "source": [
    "print(\"🔧 Starting feature selection and engineering...\")\n",
    "\n",
    "# Columns to exclude from features (non-predictive)\n",
    "exclude_columns = [\n",
    "    # ID and name columns\n",
    "    'rowid', 'kepid', 'kepoi_name', 'kepler_name',\n",
    "    \n",
    "    # Target and disposition columns\n",
    "    'koi_disposition', 'koi_pdisposition', 'target_3class',\n",
    "    \n",
    "    # Comments and metadata\n",
    "    'koi_comment', 'koi_disp_prov', 'koi_parm_prov', 'koi_sparprov',\n",
    "    \n",
    "    # Data links and deliverables\n",
    "    'koi_tce_delivname', 'koi_datalink_dvr', 'koi_datalink_dvs',\n",
    "    \n",
    "    # Transit model specific (keep koi_trans_mod, exclude detailed limb darkening)\n",
    "    'koi_limbdark_mod',\n",
    "    \n",
    "    # Date columns (keep if needed for temporal analysis)\n",
    "    'koi_vet_date'\n",
    "]\n",
    "\n",
    "# Get all column names\n",
    "all_columns = df.columns.tolist()\n",
    "print(f\"📋 Total columns in dataset: {len(all_columns)}\")\n",
    "\n",
    "# Feature columns (excluding target and non-predictive)\n",
    "feature_columns = [col for col in all_columns if col not in exclude_columns]\n",
    "print(f\"🎯 Feature columns selected: {len(feature_columns)}\")\n",
    "\n",
    "# Display excluded columns\n",
    "print(f\"\\n🗑️ Excluded columns ({len(exclude_columns)}):\")\n",
    "for col in exclude_columns:\n",
    "    if col in all_columns:\n",
    "        print(f\"   • {col}\")\n",
    "\n",
    "# Analyze feature types\n",
    "X_features = df[feature_columns].copy()\n",
    "\n",
    "print(f\"\\n📊 FEATURE ANALYSIS:\")\n",
    "print(f\"   Numerical features: {X_features.select_dtypes(include=[np.number]).shape[1]}\")\n",
    "print(f\"   Object features: {X_features.select_dtypes(include=['object']).shape[1]}\")\n",
    "\n",
    "# Check for object/string columns that might need processing\n",
    "object_cols = X_features.select_dtypes(include=['object']).columns.tolist()\n",
    "if object_cols:\n",
    "    print(f\"\\n🔤 Object columns requiring attention:\")\n",
    "    for col in object_cols:\n",
    "        unique_values = X_features[col].nunique()\n",
    "        print(f\"   • {col}: {unique_values} unique values\")\n",
    "        if unique_values <= 10:  # Show sample values for categorical\n",
    "            sample_values = X_features[col].dropna().unique()[:5]\n",
    "            print(f\"     Sample: {sample_values}\")\n",
    "\n",
    "print(f\"\\n✅ Feature selection completed!\")\n",
    "print(f\"📈 Features shape: {X_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3f3b41",
   "metadata": {},
   "source": [
    "## 4. Handle Object/Categorical Columns\n",
    "\n",
    "Process categorical and object columns in the Kepler dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23884c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔤 Processing object/categorical columns...\n",
      "📅 Processing koi_quarters...\n",
      "🔬 Processing koi_fittype...\n",
      "   Fit types: {'LS+MCMC': 7897, 'MCMC': 1206, 'none': 369, 'LS': 92}\n",
      "   Encoding: {'LS+MCMC': 0, 'MCMC': 1, 'none': 2, 'LS': 3}\n",
      "🌌 Processing koi_trans_mod...\n",
      "   Transit models: {'Mandel and Agol (2002 ApJ 580 171)': 9201}\n",
      "   Most common model: Mandel and Agol (2002 ApJ 580 171)\n",
      "🗑️ Dropped original object columns: ['koi_fittype', 'koi_trans_mod', 'koi_quarters']\n",
      "⚠️ Remaining object columns: ['koi_vet_stat']\n",
      "🗑️ Dropped remaining object columns\n",
      "\n",
      "✅ Object column processing completed!\n",
      "📈 Processed features shape: (9564, 125)\n",
      "🔢 All columns are now numerical: True\n"
     ]
    }
   ],
   "source": [
    "print(\"🔤 Processing object/categorical columns...\")\n",
    "\n",
    "# Handle specific object columns in Kepler dataset\n",
    "X_processed = X_features.copy()\n",
    "\n",
    "# Process koi_quarters (quarters when observed)\n",
    "if 'koi_quarters' in X_processed.columns:\n",
    "    print(\"📅 Processing koi_quarters...\")\n",
    "    # Convert quarters to number of quarters observed\n",
    "    X_processed['quarters_count'] = X_processed['koi_quarters'].apply(\n",
    "        lambda x: str(x).count('1') if pd.notna(x) else 0\n",
    "    )\n",
    "    # Keep original for now, will decide later\n",
    "    \n",
    "# Process koi_fittype (fitting method)\n",
    "if 'koi_fittype' in X_processed.columns:\n",
    "    print(\"🔬 Processing koi_fittype...\")\n",
    "    fittype_counts = X_processed['koi_fittype'].value_counts()\n",
    "    print(f\"   Fit types: {fittype_counts.to_dict()}\")\n",
    "    \n",
    "    # Convert to numerical encoding\n",
    "    fittype_mapping = {}\n",
    "    for i, fittype in enumerate(fittype_counts.index):\n",
    "        if pd.notna(fittype):\n",
    "            fittype_mapping[fittype] = i\n",
    "    \n",
    "    X_processed['fittype_encoded'] = X_processed['koi_fittype'].map(fittype_mapping)\n",
    "    print(f\"   Encoding: {fittype_mapping}\")\n",
    "\n",
    "# Process koi_trans_mod (transit model)\n",
    "if 'koi_trans_mod' in X_processed.columns:\n",
    "    print(\"🌌 Processing koi_trans_mod...\")\n",
    "    transmod_counts = X_processed['koi_trans_mod'].value_counts()\n",
    "    print(f\"   Transit models: {transmod_counts.to_dict()}\")\n",
    "    \n",
    "    # Most common transit model gets 1, others get 0\n",
    "    most_common_model = transmod_counts.index[0] if len(transmod_counts) > 0 else None\n",
    "    X_processed['is_mandel_agol'] = (X_processed['koi_trans_mod'] == most_common_model).astype(int)\n",
    "    print(f\"   Most common model: {most_common_model}\")\n",
    "\n",
    "# Drop original object columns after processing\n",
    "object_cols_to_drop = ['koi_fittype', 'koi_trans_mod', 'koi_quarters']\n",
    "existing_to_drop = [col for col in object_cols_to_drop if col in X_processed.columns]\n",
    "if existing_to_drop:\n",
    "    X_processed = X_processed.drop(columns=existing_to_drop)\n",
    "    print(f\"🗑️ Dropped original object columns: {existing_to_drop}\")\n",
    "\n",
    "# Check remaining object columns\n",
    "remaining_objects = X_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "if remaining_objects:\n",
    "    print(f\"⚠️ Remaining object columns: {remaining_objects}\")\n",
    "    # Drop them for now\n",
    "    X_processed = X_processed.drop(columns=remaining_objects)\n",
    "    print(f\"🗑️ Dropped remaining object columns\")\n",
    "\n",
    "print(f\"\\n✅ Object column processing completed!\")\n",
    "print(f\"📈 Processed features shape: {X_processed.shape}\")\n",
    "print(f\"🔢 All columns are now numerical: {X_processed.select_dtypes(include=[np.number]).shape[1] == X_processed.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3d609c",
   "metadata": {},
   "source": [
    "## 5. Missing Value Analysis and Handling\n",
    "\n",
    "Analyze and handle missing values in the processed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a7e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🕳️ Analyzing and handling missing values...\n",
      "📊 Columns with missing values: 111 out of 125\n",
      "\n",
      "📉 Top 15 columns with missing values:\n",
      "   koi_ingress_err1: 9564 (100.0%)\n",
      "   koi_model_chisq: 9564 (100.0%)\n",
      "   koi_longp_err1: 9564 (100.0%)\n",
      "   koi_longp: 9564 (100.0%)\n",
      "   koi_eccen_err2: 9564 (100.0%)\n",
      "   koi_eccen_err1: 9564 (100.0%)\n",
      "   koi_model_dof: 9564 (100.0%)\n",
      "   koi_ingress: 9564 (100.0%)\n",
      "   koi_ingress_err2: 9564 (100.0%)\n",
      "   koi_sage: 9564 (100.0%)\n",
      "   koi_sage_err1: 9564 (100.0%)\n",
      "   koi_sma_err1: 9564 (100.0%)\n",
      "   koi_sma_err2: 9564 (100.0%)\n",
      "   koi_sage_err2: 9564 (100.0%)\n",
      "   koi_incl_err1: 9564 (100.0%)\n",
      "\n",
      "🛠️ Missing value handling strategy:\n",
      "🗑️ Dropping 19 columns with >80% missing values\n",
      "   • koi_ingress_err1 (100.0% missing)\n",
      "   • koi_model_chisq (100.0% missing)\n",
      "   • koi_longp_err1 (100.0% missing)\n",
      "   • koi_longp (100.0% missing)\n",
      "   • koi_eccen_err2 (100.0% missing)\n",
      "   ... and 14 more\n",
      "\n",
      "🔧 Imputing 44,101 remaining missing values...\n",
      "✅ Missing values after imputation: 0\n",
      "\n",
      "📈 Final processed features shape: (9564, 106)\n",
      "🎯 Ready for model training!\n",
      "✅ Missing values after imputation: 0\n",
      "\n",
      "📈 Final processed features shape: (9564, 106)\n",
      "🎯 Ready for model training!\n"
     ]
    }
   ],
   "source": [
    "print(\"🕳️ Analyzing and handling missing values...\")\n",
    "\n",
    "# Missing value analysis\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'column': X_processed.columns,\n",
    "    'missing_count': X_processed.isnull().sum(),\n",
    "    'missing_pct': (X_processed.isnull().sum() / len(X_processed)) * 100\n",
    "}).sort_values('missing_pct', ascending=False)\n",
    "\n",
    "# Filter columns with missing values\n",
    "missing_cols = missing_analysis[missing_analysis['missing_count'] > 0]\n",
    "\n",
    "print(f\"📊 Columns with missing values: {len(missing_cols)} out of {len(X_processed.columns)}\")\n",
    "\n",
    "if len(missing_cols) > 0:\n",
    "    print(\"\\n📉 Top 15 columns with missing values:\")\n",
    "    for _, row in missing_cols.head(15).iterrows():\n",
    "        print(f\"   {row['column']}: {row['missing_count']} ({row['missing_pct']:.1f}%)\")\n",
    "    \n",
    "    # Strategy for handling missing values\n",
    "    print(\"\\n🛠️ Missing value handling strategy:\")\n",
    "    \n",
    "    # 1. Drop columns with >80% missing values\n",
    "    high_missing_cols = missing_cols[missing_cols['missing_pct'] > 80]['column'].tolist()\n",
    "    if high_missing_cols:\n",
    "        print(f\"🗑️ Dropping {len(high_missing_cols)} columns with >80% missing values\")\n",
    "        X_processed = X_processed.drop(columns=high_missing_cols)\n",
    "        for col in high_missing_cols[:5]:  # Show first 5\n",
    "            missing_pct = missing_cols[missing_cols['column'] == col]['missing_pct'].iloc[0]\n",
    "            print(f\"   • {col} ({missing_pct:.1f}% missing)\")\n",
    "        if len(high_missing_cols) > 5:\n",
    "            print(f\"   ... and {len(high_missing_cols) - 5} more\")\n",
    "    \n",
    "    # 2. Impute remaining missing values\n",
    "    remaining_missing = X_processed.isnull().sum().sum()\n",
    "    if remaining_missing > 0:\n",
    "        print(f\"\\n🔧 Imputing {remaining_missing:,} remaining missing values...\")\n",
    "        \n",
    "        # Use median imputation for numerical features\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        X_processed_imputed = pd.DataFrame(\n",
    "            imputer.fit_transform(X_processed),\n",
    "            columns=X_processed.columns,\n",
    "            index=X_processed.index\n",
    "        )\n",
    "        \n",
    "        # Verify no missing values remain\n",
    "        final_missing = X_processed_imputed.isnull().sum().sum()\n",
    "        print(f\"✅ Missing values after imputation: {final_missing}\")\n",
    "        \n",
    "        X_processed = X_processed_imputed\n",
    "    \n",
    "else:\n",
    "    print(\"✅ No missing values found!\")\n",
    "\n",
    "print(f\"\\n📈 Final processed features shape: {X_processed.shape}\")\n",
    "print(f\"🎯 Ready for model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427062b8",
   "metadata": {},
   "source": [
    "## 6. Data Scaling and Train-Test Split\n",
    "\n",
    "Split the data and apply scaling for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329cf227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚖️ Splitting data and applying scaling...\n",
      "📊 Final dataset shape: X=(9564, 106), y=(9564,)\n",
      "✅ X and y are properly aligned\n",
      "\n",
      "📊 TRAIN-TEST SPLIT RESULTS:\n",
      "   Training set: 7651 samples (80.0%)\n",
      "   Test set: 1913 samples (20.0%)\n",
      "\n",
      "📈 Class distribution in splits:\n",
      "   Train:\n",
      "     0 (Candidate): 1583 (20.7%)\n",
      "     1 (Confirmed): 2197 (28.7%)\n",
      "     2 (False_Positive): 3871 (50.6%)\n",
      "   Test:\n",
      "     0 (Candidate): 396 (20.7%)\n",
      "     1 (Confirmed): 549 (28.7%)\n",
      "     2 (False_Positive): 968 (50.6%)\n",
      "\n",
      "🔧 Applying StandardScaler...\n",
      "✅ Scaling completed!\n",
      "📊 Scaled features - Train: (7651, 106), Test: (1913, 106)\n",
      "\n",
      "📏 Scaling verification (training set):\n",
      "   Mean: 0.000000 (should be ~0)\n",
      "   Std: 0.971762 (should be ~1)\n"
     ]
    }
   ],
   "source": [
    "print(\"⚖️ Splitting data and applying scaling...\")\n",
    "\n",
    "# Prepare final dataset\n",
    "X_final = X_processed.copy()\n",
    "y_final = df['target_3class'].copy()\n",
    "\n",
    "print(f\"📊 Final dataset shape: X={X_final.shape}, y={y_final.shape}\")\n",
    "\n",
    "# Verify alignment\n",
    "assert len(X_final) == len(y_final), \"X and y must have same number of samples\"\n",
    "print(\"✅ X and y are properly aligned\")\n",
    "\n",
    "# Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y_final,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_final\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 TRAIN-TEST SPLIT RESULTS:\")\n",
    "print(f\"   Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X_final)*100:.1f}%)\")\n",
    "print(f\"   Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X_final)*100:.1f}%)\")\n",
    "\n",
    "# Check class distribution in splits\n",
    "print(f\"\\n📈 Class distribution in splits:\")\n",
    "class_names = ['Candidate', 'Confirmed', 'False_Positive']\n",
    "\n",
    "for split_name, y_split in [('Train', y_train), ('Test', y_test)]:\n",
    "    print(f\"   {split_name}:\")\n",
    "    split_counts = y_split.value_counts().sort_index()\n",
    "    for class_id, count in split_counts.items():\n",
    "        class_name = class_names[int(class_id)]\n",
    "        pct = count / len(y_split) * 100\n",
    "        print(f\"     {class_id} ({class_name}): {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Apply scaling\n",
    "print(f\"\\n🔧 Applying StandardScaler...\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data and transform both sets\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "print(f\"✅ Scaling completed!\")\n",
    "print(f\"📊 Scaled features - Train: {X_train_scaled.shape}, Test: {X_test_scaled.shape}\")\n",
    "\n",
    "# Verify scaling\n",
    "print(f\"\\n📏 Scaling verification (training set):\")\n",
    "print(f\"   Mean: {X_train_scaled.mean().mean():.6f} (should be ~0)\")\n",
    "print(f\"   Std: {X_train_scaled.std().mean():.6f} (should be ~1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69185c3",
   "metadata": {},
   "source": [
    "## 7. Export Processed Data\n",
    "\n",
    "Save the preprocessed data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3b9a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Exporting processed data...\n",
      "📂 Output directory: kepler_3class/\n",
      "💾 Saving train-test splits...\n",
      "💾 Saving full processed dataset...\n",
      "💾 Saving full processed dataset...\n",
      "💾 Saving scaler...\n",
      "💾 Saving target mapping...\n",
      "💾 Saving metadata...\n",
      "\n",
      "✅ PREPROCESSING COMPLETED SUCCESSFULLY!\n",
      "📂 All files saved to: kepler_3class/\n",
      "\n",
      "📋 Files created:\n",
      "   • X_train_scaled.csv - Scaled training features ((7651, 106))\n",
      "   • X_test_scaled.csv - Scaled test features ((1913, 106))\n",
      "   • y_train.csv - Training targets (7651 samples)\n",
      "   • y_test.csv - Test targets (1913 samples)\n",
      "   • X_final_cleaned.csv - Full feature matrix ((9564, 106))\n",
      "   • y_final_cleaned.csv - Full target vector (9564 samples)\n",
      "   • scaler.joblib - Fitted StandardScaler\n",
      "   • target_mapping.json - Class mappings and descriptions\n",
      "   • metadata.json - Complete preprocessing metadata\n",
      "\n",
      "🎯 READY FOR MODEL TRAINING!\n",
      "📊 Dataset: 9564 samples, 106 features, 3 classes\n",
      "🏆 Class distribution: {0: 1979, 1: 2746, 2: 4839}\n",
      "⚖️ Class imbalance ratio: 2.4:1\n",
      "💾 Saving scaler...\n",
      "💾 Saving target mapping...\n",
      "💾 Saving metadata...\n",
      "\n",
      "✅ PREPROCESSING COMPLETED SUCCESSFULLY!\n",
      "📂 All files saved to: kepler_3class/\n",
      "\n",
      "📋 Files created:\n",
      "   • X_train_scaled.csv - Scaled training features ((7651, 106))\n",
      "   • X_test_scaled.csv - Scaled test features ((1913, 106))\n",
      "   • y_train.csv - Training targets (7651 samples)\n",
      "   • y_test.csv - Test targets (1913 samples)\n",
      "   • X_final_cleaned.csv - Full feature matrix ((9564, 106))\n",
      "   • y_final_cleaned.csv - Full target vector (9564 samples)\n",
      "   • scaler.joblib - Fitted StandardScaler\n",
      "   • target_mapping.json - Class mappings and descriptions\n",
      "   • metadata.json - Complete preprocessing metadata\n",
      "\n",
      "🎯 READY FOR MODEL TRAINING!\n",
      "📊 Dataset: 9564 samples, 106 features, 3 classes\n",
      "🏆 Class distribution: {0: 1979, 1: 2746, 2: 4839}\n",
      "⚖️ Class imbalance ratio: 2.4:1\n"
     ]
    }
   ],
   "source": [
    "print(\"💾 Exporting processed data...\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = 'kepler_3class'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"📂 Output directory: {output_dir}/\")\n",
    "\n",
    "# Save train-test splits\n",
    "print(\"💾 Saving train-test splits...\")\n",
    "X_train_scaled.to_csv(f'{output_dir}/X_train_scaled.csv', index=False)\n",
    "X_test_scaled.to_csv(f'{output_dir}/X_test_scaled.csv', index=False)\n",
    "y_train.to_csv(f'{output_dir}/y_train.csv', index=False)\n",
    "y_test.to_csv(f'{output_dir}/y_test.csv', index=False)\n",
    "\n",
    "# Save full processed dataset\n",
    "print(\"💾 Saving full processed dataset...\")\n",
    "X_final.to_csv(f'{output_dir}/X_final_cleaned.csv', index=False)\n",
    "y_final.to_csv(f'{output_dir}/y_final_cleaned.csv', index=False)\n",
    "\n",
    "# Save scaler\n",
    "print(\"💾 Saving scaler...\")\n",
    "joblib.dump(scaler, f'{output_dir}/scaler.joblib')\n",
    "\n",
    "# Save target mapping\n",
    "print(\"💾 Saving target mapping...\")\n",
    "with open(f'{output_dir}/target_mapping.json', 'w') as f:\n",
    "    json.dump(target_mapping, f, indent=2)\n",
    "\n",
    "# Create metadata\n",
    "metadata = {\n",
    "    'dataset_name': 'Kepler Exoplanet 3-Class',\n",
    "    'original_shape': df.shape,\n",
    "    'final_samples': len(X_final),\n",
    "    'features': len(X_final.columns),\n",
    "    'target_classes': 3,\n",
    "    'class_distribution': y_final.value_counts().sort_index().to_dict(),\n",
    "    'train_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'feature_names': X_final.columns.tolist(),\n",
    "    'preprocessing_steps': [\n",
    "        'Target creation (3-class)',\n",
    "        'Feature selection and engineering',\n",
    "        'Object column processing',\n",
    "        'Missing value imputation',\n",
    "        'Standard scaling',\n",
    "        'Train-test split (80/20)'\n",
    "    ],\n",
    "    'preprocessing_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'class_descriptions': target_mapping['class_descriptions']\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "print(\"💾 Saving metadata...\")\n",
    "with open(f'{output_dir}/metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "# Summary of exported files\n",
    "print(f\"\\n✅ PREPROCESSING COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"📂 All files saved to: {output_dir}/\")\n",
    "print(f\"\\n📋 Files created:\")\n",
    "print(f\"   • X_train_scaled.csv - Scaled training features ({X_train_scaled.shape})\")\n",
    "print(f\"   • X_test_scaled.csv - Scaled test features ({X_test_scaled.shape})\")\n",
    "print(f\"   • y_train.csv - Training targets ({len(y_train)} samples)\")\n",
    "print(f\"   • y_test.csv - Test targets ({len(y_test)} samples)\")\n",
    "print(f\"   • X_final_cleaned.csv - Full feature matrix ({X_final.shape})\")\n",
    "print(f\"   • y_final_cleaned.csv - Full target vector ({len(y_final)} samples)\")\n",
    "print(f\"   • scaler.joblib - Fitted StandardScaler\")\n",
    "print(f\"   • target_mapping.json - Class mappings and descriptions\")\n",
    "print(f\"   • metadata.json - Complete preprocessing metadata\")\n",
    "\n",
    "print(f\"\\n🎯 READY FOR MODEL TRAINING!\")\n",
    "print(f\"📊 Dataset: {len(X_final)} samples, {len(X_final.columns)} features, 3 classes\")\n",
    "print(f\"🏆 Class distribution: {dict(y_final.value_counts().sort_index())}\")\n",
    "print(f\"⚖️ Class imbalance ratio: {y_final.value_counts().max() / y_final.value_counts().min():.1f}:1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
