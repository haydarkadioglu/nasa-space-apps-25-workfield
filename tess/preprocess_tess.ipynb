{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd64f2f",
   "metadata": {},
   "source": [
    "# TESS Exoplanet Data Preprocessing Pipeline - 3-Class Classification\n",
    "\n",
    "This notebook implements a comprehensive preprocessing pipeline for the TESS Objects of Interest (TOI) dataset for 3-class classification.\n",
    "\n",
    "## Dataset Information:\n",
    "- **Source**: TESS Objects of Interest (TOI) Catalog\n",
    "- **Size**: 7,703 objects Ã— 87 features\n",
    "- **Target Classes**: PC (Planet Candidate), CP (Confirmed Planet), FP (False Positive)\n",
    "\n",
    "## Target Distribution:\n",
    "- **PC (Planet Candidate)**: 4,679 objects\n",
    "- **FP (False Positive)**: 1,197 objects  \n",
    "- **CP (Confirmed Planet)**: 684 objects\n",
    "- **Other categories**: KP, APC, FA (will be grouped appropriately)\n",
    "\n",
    "## Preprocessing Steps:\n",
    "1. **Data Loading and Initial Exploration** - Load CSV, check dimensions, data types, and missing values\n",
    "2. **Target Variable Creation** - 3-class classification: Confirmed vs Candidate vs False Positive\n",
    "3. **Feature Selection and Engineering** - Process TESS-specific features\n",
    "4. **Missing Value Handling** - Clean and impute missing data\n",
    "5. **Data Scaling and Splitting** - Prepare for model training\n",
    "6. **Export Processed Data** - Save for model training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97a0601f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Libraries imported successfully!\n",
      "ğŸš€ Ready for TESS data preprocessing!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options for better output formatting\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"ğŸ“š Libraries imported successfully!\")\n",
    "print(\"ğŸš€ Ready for TESS data preprocessing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231736cb",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration\n",
    "\n",
    "Load the TESS TOI CSV file and perform initial data exploration including:\n",
    "- Dataset dimensions (rows/columns)\n",
    "- Data types examination\n",
    "- Missing value analysis\n",
    "- Target distribution analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53cb9a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESS TOI DATASET OVERVIEW ===\n",
      "ğŸ“Š Dataset shape: 7703 rows Ã— 87 columns\n",
      "ğŸ’¾ Memory usage: 7.10 MB\n",
      "\n",
      "=== TARGET DISTRIBUTION (TFOPWG_DISP) ===\n",
      "tfopwg_disp\n",
      "PC     4679\n",
      "FP     1197\n",
      "CP      684\n",
      "KP      583\n",
      "APC     462\n",
      "FA       98\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ“ˆ Target percentages:\n",
      "   PC: 4679 (60.7%)\n",
      "   FP: 1197 (15.5%)\n",
      "   CP: 684 (8.9%)\n",
      "   KP: 583 (7.6%)\n",
      "   APC: 462 (6.0%)\n",
      "   FA: 98 (1.3%)\n",
      "\n",
      "ğŸ“‹ TESS DISPOSITION CATEGORIES:\n",
      "   â€¢ PC: Planet Candidate - Objects that pass initial vetting (4679 objects)\n",
      "   â€¢ CP: Confirmed Planet - Objects confirmed as exoplanets (684 objects)\n",
      "   â€¢ FP: False Positive - Objects determined to be false alarms (1197 objects)\n",
      "   â€¢ KP: Known Planet - Previously known planets in TOI catalog (583 objects)\n",
      "   â€¢ APC: Ambiguous Planet Candidate - Uncertain classification (462 objects)\n",
      "   â€¢ FA: False Alarm - Clear false detections (98 objects)\n",
      "\n",
      "=== DATA TYPES SUMMARY ===\n",
      "float64    58\n",
      "int64      24\n",
      "object      5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== MISSING VALUES ANALYSIS ===\n",
      "Total missing values: 111,013 (16.57%)\n",
      "\n",
      "ğŸ“‰ Top 10 columns with missing values:\n",
      "   decerr2: 7703 (100.0%)\n",
      "   pl_eqterr1: 7703 (100.0%)\n",
      "   pl_insolsymerr: 7703 (100.0%)\n",
      "   decerr1: 7703 (100.0%)\n",
      "   pl_insolerr2: 7703 (100.0%)\n",
      "   raerr2: 7703 (100.0%)\n",
      "   pl_insolerr1: 7703 (100.0%)\n",
      "   raerr1: 7703 (100.0%)\n",
      "   pl_insollim: 7703 (100.0%)\n",
      "   pl_eqterr2: 7703 (100.0%)\n",
      "\n",
      "=== SAMPLE DATA (First 3 rows, selected columns) ===\n",
      "       toi tfopwg_disp          ra        dec  pl_orbper    pl_rade   st_rad  \\\n",
      "0  1000.01          FP  112.357708 -12.695960   2.171348   5.818163  2.16986   \n",
      "1  1001.01          PC  122.580465  -5.513852   1.931646  11.215400  2.01000   \n",
      "2  1002.01          FP  104.726966 -10.580455   1.867557  23.752900  5.73000   \n",
      "\n",
      "   st_teff  \n",
      "0  10249.0  \n",
      "1   7070.0  \n",
      "2   8924.0  \n"
     ]
    }
   ],
   "source": [
    "# Load the TESS TOI dataset\n",
    "df = pd.read_csv('TOI.csv')\n",
    "\n",
    "# Basic dataset information\n",
    "print(\"=== TESS TOI DATASET OVERVIEW ===\")\n",
    "print(f\"ğŸ“Š Dataset shape: {df.shape[0]} rows Ã— {df.shape[1]} columns\")\n",
    "print(f\"ğŸ’¾ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Target variable analysis\n",
    "print(\"\\n=== TARGET DISTRIBUTION (TFOPWG_DISP) ===\")\n",
    "target_counts = df['tfopwg_disp'].value_counts()\n",
    "print(target_counts)\n",
    "print(f\"\\nğŸ“ˆ Target percentages:\")\n",
    "for disposition, count in target_counts.items():\n",
    "    pct = count / len(df) * 100\n",
    "    print(f\"   {disposition}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Explain TESS categories\n",
    "print(\"\\nğŸ“‹ TESS DISPOSITION CATEGORIES:\")\n",
    "tess_categories = {\n",
    "    'PC': 'Planet Candidate - Objects that pass initial vetting',\n",
    "    'CP': 'Confirmed Planet - Objects confirmed as exoplanets',\n",
    "    'FP': 'False Positive - Objects determined to be false alarms',\n",
    "    'KP': 'Known Planet - Previously known planets in TOI catalog',\n",
    "    'APC': 'Ambiguous Planet Candidate - Uncertain classification',\n",
    "    'FA': 'False Alarm - Clear false detections'\n",
    "}\n",
    "\n",
    "for category, description in tess_categories.items():\n",
    "    if category in target_counts.index:\n",
    "        count = target_counts[category]\n",
    "        print(f\"   â€¢ {category}: {description} ({count} objects)\")\n",
    "\n",
    "# Data types overview\n",
    "print(\"\\n=== DATA TYPES SUMMARY ===\")\n",
    "dtype_counts = df.dtypes.value_counts()\n",
    "print(dtype_counts)\n",
    "\n",
    "# Missing values analysis\n",
    "print(\"\\n=== MISSING VALUES ANALYSIS ===\")\n",
    "missing_total = df.isnull().sum().sum()\n",
    "missing_pct = (missing_total / (df.shape[0] * df.shape[1])) * 100\n",
    "print(f\"Total missing values: {missing_total:,} ({missing_pct:.2f}%)\")\n",
    "\n",
    "# Columns with highest missing values\n",
    "missing_by_col = df.isnull().sum().sort_values(ascending=False)\n",
    "top_missing = missing_by_col[missing_by_col > 0].head(10)\n",
    "if not top_missing.empty:\n",
    "    print(\"\\nğŸ“‰ Top 10 columns with missing values:\")\n",
    "    for col, missing_count in top_missing.items():\n",
    "        missing_pct_col = (missing_count / len(df)) * 100\n",
    "        print(f\"   {col}: {missing_count} ({missing_pct_col:.1f}%)\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\n=== SAMPLE DATA (First 3 rows, selected columns) ===\")\n",
    "sample_cols = ['toi', 'tfopwg_disp', 'ra', 'dec', 'pl_orbper', 'pl_trandur', 'pl_rade', 'st_rad', 'st_teff']\n",
    "available_cols = [col for col in sample_cols if col in df.columns]\n",
    "print(df[available_cols].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dcb379",
   "metadata": {},
   "source": [
    "## 2. Target Variable Creation - 3-Class Classification\n",
    "\n",
    "Convert the TESS disposition categories into a 3-class classification problem:\n",
    "- **Candidate (0)**: Planet candidates needing follow-up (PC, APC)\n",
    "- **Confirmed (1)**: Confirmed and known planets (CP, KP)  \n",
    "- **False_Positive (2)**: False positives and false alarms (FP, FA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b82d35fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Creating 3-class target variable...\n",
      "\n",
      "ğŸ“Š DISPOSITION TO 3-CLASS MAPPING:\n",
      "   APC â†’ 0 (Candidate): 462 objects\n",
      "   CP â†’ 1 (Confirmed): 684 objects\n",
      "   FA â†’ 2 (False_Positive): 98 objects\n",
      "   FP â†’ 2 (False_Positive): 1197 objects\n",
      "   KP â†’ 1 (Confirmed): 583 objects\n",
      "   PC â†’ 0 (Candidate): 4679 objects\n",
      "\n",
      "ğŸ“ˆ FINAL 3-CLASS TARGET DISTRIBUTION:\n",
      "   0 (Candidate): 5141 (66.7%)\n",
      "   1 (Confirmed): 1267 (16.4%)\n",
      "   2 (False_Positive): 1295 (16.8%)\n",
      "\n",
      "âš–ï¸ Class imbalance ratio: 4.1:1\n",
      "\n",
      "âœ… Target variable created successfully!\n",
      "ğŸ“ˆ Final dataset shape: (7703, 88)\n"
     ]
    }
   ],
   "source": [
    "# Create 3-class target variable\n",
    "print(\"ğŸ¯ Creating 3-class target variable...\")\n",
    "\n",
    "# Map TESS dispositions to 3 classes\n",
    "disposition_mapping = {\n",
    "    'PC': 0,    # Planet Candidate\n",
    "    'APC': 0,   # Ambiguous Planet Candidate\n",
    "    'CP': 1,    # Confirmed Planet\n",
    "    'KP': 1,    # Known Planet  \n",
    "    'FP': 2,    # False Positive\n",
    "    'FA': 2     # False Alarm\n",
    "}\n",
    "\n",
    "# Create target variable\n",
    "df['target_3class'] = df['tfopwg_disp'].map(disposition_mapping)\n",
    "\n",
    "# Check for any unmapped values\n",
    "unmapped = df[df['target_3class'].isnull()]['tfopwg_disp'].unique()\n",
    "if len(unmapped) > 0:\n",
    "    print(f\"âš ï¸ Unmapped dispositions found: {unmapped}\")\n",
    "    # Handle any edge cases here if needed\n",
    "    \n",
    "# Remove rows with missing target\n",
    "initial_count = len(df)\n",
    "df = df.dropna(subset=['target_3class'])\n",
    "removed_count = initial_count - len(df)\n",
    "if removed_count > 0:\n",
    "    print(f\"ğŸ—‘ï¸ Removed {removed_count} rows with missing target\")\n",
    "\n",
    "# Show original to 3-class mapping\n",
    "print(\"\\nğŸ“Š DISPOSITION TO 3-CLASS MAPPING:\")\n",
    "mapping_summary = df.groupby(['tfopwg_disp', 'target_3class']).size().reset_index(name='count')\n",
    "class_names = ['Candidate', 'Confirmed', 'False_Positive']\n",
    "\n",
    "for _, row in mapping_summary.iterrows():\n",
    "    original_disp = row['tfopwg_disp']\n",
    "    target_class = int(row['target_3class'])\n",
    "    count = row['count']\n",
    "    class_name = class_names[target_class]\n",
    "    print(f\"   {original_disp} â†’ {target_class} ({class_name}): {count} objects\")\n",
    "\n",
    "# Final target distribution\n",
    "print(\"\\nğŸ“ˆ FINAL 3-CLASS TARGET DISTRIBUTION:\")\n",
    "target_counts = df['target_3class'].value_counts().sort_index()\n",
    "\n",
    "for class_id, count in target_counts.items():\n",
    "    class_name = class_names[int(class_id)]\n",
    "    pct = count / len(df) * 100\n",
    "    print(f\"   {class_id} ({class_name}): {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Class imbalance analysis\n",
    "class_counts = target_counts.values\n",
    "imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "print(f\"\\nâš–ï¸ Class imbalance ratio: {imbalance_ratio:.1f}:1\")\n",
    "\n",
    "# Create target mapping for reference\n",
    "target_mapping = {\n",
    "    'original_mapping': disposition_mapping,\n",
    "    'encoding': {str(k): v for v, k in enumerate(class_names)},\n",
    "    'class_descriptions': {\n",
    "        'Candidate': 'Planet candidates and ambiguous candidates requiring follow-up observation',\n",
    "        'Confirmed': 'Confirmed exoplanets and known planets with high confidence',\n",
    "        'False_Positive': 'False positives and false alarms determined to be non-planetary'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nâœ… Target variable created successfully!\")\n",
    "print(f\"ğŸ“ˆ Final dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79228157",
   "metadata": {},
   "source": [
    "## 3. Feature Selection and Engineering\n",
    "\n",
    "Select and engineer relevant features from the TESS dataset:\n",
    "- Remove non-predictive columns (IDs, names, comments)\n",
    "- Process numerical features\n",
    "- Handle string columns and create derived features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d1acef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Starting feature selection and engineering...\n",
      "ğŸ“‹ Total columns in dataset: 88\n",
      "ğŸ¯ Feature columns selected: 76\n",
      "\n",
      "ğŸ—‘ï¸ Excluded columns (15):\n",
      "   â€¢ rowid\n",
      "   â€¢ toi\n",
      "   â€¢ toipfx\n",
      "   â€¢ tid\n",
      "   â€¢ ctoi_alias\n",
      "   â€¢ pl_pnum\n",
      "   â€¢ tfopwg_disp\n",
      "   â€¢ target_3class\n",
      "   â€¢ rastr\n",
      "   â€¢ decstr\n",
      "   â€¢ toi_created\n",
      "   â€¢ rowupdate\n",
      "\n",
      "ğŸ“Š FEATURE ANALYSIS:\n",
      "   Numerical features: 76\n",
      "   Object features: 0\n",
      "\n",
      "âœ… Feature selection completed!\n",
      "ğŸ“ˆ Features shape: (7703, 76)\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”§ Starting feature selection and engineering...\")\n",
    "\n",
    "# Columns to exclude from features (non-predictive)\n",
    "exclude_columns = [\n",
    "    # ID and name columns\n",
    "    'rowid', 'toi', 'toipfx', 'tid', 'ctoi_alias', 'pl_pnum',\n",
    "    \n",
    "    # Target and disposition columns\n",
    "    'tfopwg_disp', 'target_3class',\n",
    "    \n",
    "    # String identifier columns\n",
    "    'rastr', 'decstr', 'toiurl',\n",
    "    \n",
    "    # Date/time columns (keep if needed for temporal analysis)\n",
    "    'toi_created', 'rowupdate',\n",
    "    \n",
    "    # Notes and comments\n",
    "    'tfopwg_tag', 'toi_tag'\n",
    "]\n",
    "\n",
    "# Get all column names\n",
    "all_columns = df.columns.tolist()\n",
    "print(f\"ğŸ“‹ Total columns in dataset: {len(all_columns)}\")\n",
    "\n",
    "# Feature columns (excluding target and non-predictive)\n",
    "feature_columns = [col for col in all_columns if col not in exclude_columns]\n",
    "print(f\"ğŸ¯ Feature columns selected: {len(feature_columns)}\")\n",
    "\n",
    "# Display excluded columns\n",
    "print(f\"\\nğŸ—‘ï¸ Excluded columns ({len(exclude_columns)}):\")\n",
    "for col in exclude_columns:\n",
    "    if col in all_columns:\n",
    "        print(f\"   â€¢ {col}\")\n",
    "\n",
    "# Analyze feature types\n",
    "X_features = df[feature_columns].copy()\n",
    "\n",
    "print(f\"\\nğŸ“Š FEATURE ANALYSIS:\")\n",
    "print(f\"   Numerical features: {X_features.select_dtypes(include=[np.number]).shape[1]}\")\n",
    "print(f\"   Object features: {X_features.select_dtypes(include=['object']).shape[1]}\")\n",
    "\n",
    "# Check for object/string columns that might need processing\n",
    "object_cols = X_features.select_dtypes(include=['object']).columns.tolist()\n",
    "if object_cols:\n",
    "    print(f\"\\nğŸ”¤ Object columns requiring attention:\")\n",
    "    for col in object_cols:\n",
    "        unique_values = X_features[col].nunique()\n",
    "        print(f\"   â€¢ {col}: {unique_values} unique values\")\n",
    "        if unique_values <= 10:  # Show sample values for categorical\n",
    "            sample_values = X_features[col].dropna().unique()[:5]\n",
    "            print(f\"     Sample: {sample_values}\")\n",
    "\n",
    "print(f\"\\nâœ… Feature selection completed!\")\n",
    "print(f\"ğŸ“ˆ Features shape: {X_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6259a0",
   "metadata": {},
   "source": [
    "## 4. Handle Object/Categorical Columns\n",
    "\n",
    "Process categorical and object columns in the TESS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59be82ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¤ Processing object/categorical columns...\n",
      "\n",
      "âœ… Object column processing completed!\n",
      "ğŸ“ˆ Processed features shape: (7703, 76)\n",
      "ğŸ”¢ All columns are now numerical: True\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”¤ Processing object/categorical columns...\")\n",
    "\n",
    "# Handle specific object columns in TESS dataset\n",
    "X_processed = X_features.copy()\n",
    "\n",
    "# Get object columns\n",
    "object_cols = X_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "if object_cols:\n",
    "    print(f\"ğŸ“‹ Processing {len(object_cols)} object columns...\")\n",
    "    \n",
    "    for col in object_cols:\n",
    "        print(f\"\\nğŸ” Processing {col}:\")\n",
    "        unique_vals = X_processed[col].nunique()\n",
    "        print(f\"   Unique values: {unique_vals}\")\n",
    "        \n",
    "        if unique_vals <= 1:\n",
    "            # Constant column - drop it\n",
    "            print(f\"   âŒ Dropping constant column\")\n",
    "            X_processed = X_processed.drop(columns=[col])\n",
    "            \n",
    "        elif unique_vals <= 20:\n",
    "            # Categorical with few values - encode\n",
    "            print(f\"   ğŸ·ï¸ Categorical encoding\")\n",
    "            value_counts = X_processed[col].value_counts()\n",
    "            print(f\"   Values: {value_counts.to_dict()}\")\n",
    "            \n",
    "            # Create numerical encoding\n",
    "            encoding_map = {}\n",
    "            for i, value in enumerate(value_counts.index):\n",
    "                if pd.notna(value):\n",
    "                    encoding_map[value] = i\n",
    "            \n",
    "            # Apply encoding\n",
    "            X_processed[f'{col}_encoded'] = X_processed[col].map(encoding_map)\n",
    "            X_processed = X_processed.drop(columns=[col])\n",
    "            print(f\"   âœ… Created {col}_encoded\")\n",
    "            \n",
    "        else:\n",
    "            # Too many unique values - likely need special handling or drop\n",
    "            print(f\"   âš ï¸ Too many unique values ({unique_vals}) - dropping\")\n",
    "            X_processed = X_processed.drop(columns=[col])\n",
    "\n",
    "# Check remaining object columns\n",
    "remaining_objects = X_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "if remaining_objects:\n",
    "    print(f\"\\nâš ï¸ Remaining object columns: {remaining_objects}\")\n",
    "    # Drop them for now\n",
    "    X_processed = X_processed.drop(columns=remaining_objects)\n",
    "    print(f\"ğŸ—‘ï¸ Dropped remaining object columns\")\n",
    "\n",
    "print(f\"\\nâœ… Object column processing completed!\")\n",
    "print(f\"ğŸ“ˆ Processed features shape: {X_processed.shape}\")\n",
    "print(f\"ğŸ”¢ All columns are now numerical: {X_processed.select_dtypes(include=[np.number]).shape[1] == X_processed.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ac59e6",
   "metadata": {},
   "source": [
    "## 5. Missing Value Analysis and Handling\n",
    "\n",
    "Analyze and handle missing values in the processed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af5dfdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ•³ï¸ Analyzing and handling missing values...\n",
      "ğŸ“Š Columns with missing values: 48 out of 76\n",
      "\n",
      "ğŸ“‰ Top 15 columns with missing values:\n",
      "   pl_insolsymerr: 7703 (100.0%)\n",
      "   raerr2: 7703 (100.0%)\n",
      "   decerr1: 7703 (100.0%)\n",
      "   decerr2: 7703 (100.0%)\n",
      "   pl_eqtsymerr: 7703 (100.0%)\n",
      "   pl_eqtlim: 7703 (100.0%)\n",
      "   pl_eqterr2: 7703 (100.0%)\n",
      "   pl_eqterr1: 7703 (100.0%)\n",
      "   pl_insollim: 7703 (100.0%)\n",
      "   pl_insolerr2: 7703 (100.0%)\n",
      "   pl_insolerr1: 7703 (100.0%)\n",
      "   raerr1: 7703 (100.0%)\n",
      "   st_loggerr1: 2271 (29.5%)\n",
      "   st_loggerr2: 2271 (29.5%)\n",
      "   st_raderr2: 1963 (25.5%)\n",
      "\n",
      "ğŸ› ï¸ Missing value handling strategy:\n",
      "ğŸ—‘ï¸ Dropping 12 columns with >85% missing values\n",
      "   â€¢ pl_insolsymerr (100.0% missing)\n",
      "   â€¢ raerr2 (100.0% missing)\n",
      "   â€¢ decerr1 (100.0% missing)\n",
      "   â€¢ decerr2 (100.0% missing)\n",
      "   â€¢ pl_eqtsymerr (100.0% missing)\n",
      "   ... and 7 more\n",
      "\n",
      "ğŸ”§ Imputing 18,577 remaining missing values...\n",
      "âœ… Missing values after imputation: 0\n",
      "\n",
      "ğŸ“ˆ Final processed features shape: (7703, 64)\n",
      "ğŸ¯ Ready for model training!\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ•³ï¸ Analyzing and handling missing values...\")\n",
    "\n",
    "# Missing value analysis\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'column': X_processed.columns,\n",
    "    'missing_count': X_processed.isnull().sum(),\n",
    "    'missing_pct': (X_processed.isnull().sum() / len(X_processed)) * 100\n",
    "}).sort_values('missing_pct', ascending=False)\n",
    "\n",
    "# Filter columns with missing values\n",
    "missing_cols = missing_analysis[missing_analysis['missing_count'] > 0]\n",
    "\n",
    "print(f\"ğŸ“Š Columns with missing values: {len(missing_cols)} out of {len(X_processed.columns)}\")\n",
    "\n",
    "if len(missing_cols) > 0:\n",
    "    print(\"\\nğŸ“‰ Top 15 columns with missing values:\")\n",
    "    for _, row in missing_cols.head(15).iterrows():\n",
    "        print(f\"   {row['column']}: {row['missing_count']} ({row['missing_pct']:.1f}%)\")\n",
    "    \n",
    "    # Strategy for handling missing values\n",
    "    print(\"\\nğŸ› ï¸ Missing value handling strategy:\")\n",
    "    \n",
    "    # 1. Drop columns with >85% missing values\n",
    "    high_missing_cols = missing_cols[missing_cols['missing_pct'] > 85]['column'].tolist()\n",
    "    if high_missing_cols:\n",
    "        print(f\"ğŸ—‘ï¸ Dropping {len(high_missing_cols)} columns with >85% missing values\")\n",
    "        X_processed = X_processed.drop(columns=high_missing_cols)\n",
    "        for col in high_missing_cols[:5]:  # Show first 5\n",
    "            missing_pct = missing_cols[missing_cols['column'] == col]['missing_pct'].iloc[0]\n",
    "            print(f\"   â€¢ {col} ({missing_pct:.1f}% missing)\")\n",
    "        if len(high_missing_cols) > 5:\n",
    "            print(f\"   ... and {len(high_missing_cols) - 5} more\")\n",
    "    \n",
    "    # 2. Impute remaining missing values\n",
    "    remaining_missing = X_processed.isnull().sum().sum()\n",
    "    if remaining_missing > 0:\n",
    "        print(f\"\\nğŸ”§ Imputing {remaining_missing:,} remaining missing values...\")\n",
    "        \n",
    "        # Use median imputation for numerical features\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        X_processed_imputed = pd.DataFrame(\n",
    "            imputer.fit_transform(X_processed),\n",
    "            columns=X_processed.columns,\n",
    "            index=X_processed.index\n",
    "        )\n",
    "        \n",
    "        # Verify no missing values remain\n",
    "        final_missing = X_processed_imputed.isnull().sum().sum()\n",
    "        print(f\"âœ… Missing values after imputation: {final_missing}\")\n",
    "        \n",
    "        X_processed = X_processed_imputed\n",
    "    \n",
    "else:\n",
    "    print(\"âœ… No missing values found!\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Final processed features shape: {X_processed.shape}\")\n",
    "print(f\"ğŸ¯ Ready for model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da232508",
   "metadata": {},
   "source": [
    "## 6. Data Scaling and Train-Test Split\n",
    "\n",
    "Split the data and apply scaling for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95853335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš–ï¸ Splitting data and applying scaling...\n",
      "ğŸ“Š Final dataset shape: X=(7703, 64), y=(7703,)\n",
      "âœ… X and y are properly aligned\n",
      "\n",
      "ğŸ“Š TRAIN-TEST SPLIT RESULTS:\n",
      "   Training set: 6162 samples (80.0%)\n",
      "   Test set: 1541 samples (20.0%)\n",
      "\n",
      "ğŸ“ˆ Class distribution in splits:\n",
      "   Train:\n",
      "     0 (Candidate): 4112 (66.7%)\n",
      "     1 (Confirmed): 1014 (16.5%)\n",
      "     2 (False_Positive): 1036 (16.8%)\n",
      "   Test:\n",
      "     0 (Candidate): 1029 (66.8%)\n",
      "     1 (Confirmed): 253 (16.4%)\n",
      "     2 (False_Positive): 259 (16.8%)\n",
      "\n",
      "ğŸ”§ Applying StandardScaler...\n",
      "âœ… Scaling completed!\n",
      "ğŸ“Š Scaled features - Train: (6162, 64), Test: (1541, 64)\n",
      "\n",
      "ğŸ“ Scaling verification (training set):\n",
      "   Mean: -0.000000 (should be ~0)\n",
      "   Std: 0.625051 (should be ~1)\n"
     ]
    }
   ],
   "source": [
    "print(\"âš–ï¸ Splitting data and applying scaling...\")\n",
    "\n",
    "# Prepare final dataset\n",
    "X_final = X_processed.copy()\n",
    "y_final = df['target_3class'].copy()\n",
    "\n",
    "print(f\"ğŸ“Š Final dataset shape: X={X_final.shape}, y={y_final.shape}\")\n",
    "\n",
    "# Verify alignment\n",
    "assert len(X_final) == len(y_final), \"X and y must have same number of samples\"\n",
    "print(\"âœ… X and y are properly aligned\")\n",
    "\n",
    "# Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y_final,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_final\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š TRAIN-TEST SPLIT RESULTS:\")\n",
    "print(f\"   Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X_final)*100:.1f}%)\")\n",
    "print(f\"   Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X_final)*100:.1f}%)\")\n",
    "\n",
    "# Check class distribution in splits\n",
    "print(f\"\\nğŸ“ˆ Class distribution in splits:\")\n",
    "class_names = ['Candidate', 'Confirmed', 'False_Positive']\n",
    "\n",
    "for split_name, y_split in [('Train', y_train), ('Test', y_test)]:\n",
    "    print(f\"   {split_name}:\")\n",
    "    split_counts = y_split.value_counts().sort_index()\n",
    "    for class_id, count in split_counts.items():\n",
    "        class_name = class_names[int(class_id)]\n",
    "        pct = count / len(y_split) * 100\n",
    "        print(f\"     {class_id} ({class_name}): {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Apply scaling\n",
    "print(f\"\\nğŸ”§ Applying StandardScaler...\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data and transform both sets\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "print(f\"âœ… Scaling completed!\")\n",
    "print(f\"ğŸ“Š Scaled features - Train: {X_train_scaled.shape}, Test: {X_test_scaled.shape}\")\n",
    "\n",
    "# Verify scaling\n",
    "print(f\"\\nğŸ“ Scaling verification (training set):\")\n",
    "print(f\"   Mean: {X_train_scaled.mean().mean():.6f} (should be ~0)\")\n",
    "print(f\"   Std: {X_train_scaled.std().mean():.6f} (should be ~1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bb6ad8",
   "metadata": {},
   "source": [
    "## 7. Export Processed Data\n",
    "\n",
    "Save the preprocessed data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91323df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Exporting processed data...\n",
      "ğŸ“‚ Output directory: tess_3class/\n",
      "ğŸ’¾ Saving train-test splits...\n",
      "ğŸ’¾ Saving full processed dataset...\n",
      "ğŸ’¾ Saving scaler...\n",
      "ğŸ’¾ Saving target mapping...\n",
      "ğŸ’¾ Saving metadata...\n",
      "\n",
      "âœ… PREPROCESSING COMPLETED SUCCESSFULLY!\n",
      "ğŸ“‚ All files saved to: tess_3class/\n",
      "\n",
      "ğŸ“‹ Files created:\n",
      "   â€¢ X_train_scaled.csv - Scaled training features ((6162, 64))\n",
      "   â€¢ X_test_scaled.csv - Scaled test features ((1541, 64))\n",
      "   â€¢ y_train.csv - Training targets (6162 samples)\n",
      "   â€¢ y_test.csv - Test targets (1541 samples)\n",
      "   â€¢ X_final_cleaned.csv - Full feature matrix ((7703, 64))\n",
      "   â€¢ y_final_cleaned.csv - Full target vector (7703 samples)\n",
      "   â€¢ scaler.joblib - Fitted StandardScaler\n",
      "   â€¢ target_mapping.json - Class mappings and descriptions\n",
      "   â€¢ metadata.json - Complete preprocessing metadata\n",
      "\n",
      "ğŸ¯ READY FOR MODEL TRAINING!\n",
      "ğŸ“Š Dataset: 7703 samples, 64 features, 3 classes\n",
      "ğŸ† Class distribution: {0: 5141, 1: 1267, 2: 1295}\n",
      "âš–ï¸ Class imbalance ratio: 4.1:1\n",
      "\n",
      "ğŸŒŸ TESS-SPECIFIC INFO:\n",
      "ğŸ“¡ Source: TESS Objects of Interest (TOI) Catalog\n",
      "ğŸ¯ 3-Class Mapping:\n",
      "   â€¢ Candidate (0): PC, APC - Planet candidates needing follow-up\n",
      "   â€¢ Confirmed (1): CP, KP - Confirmed and known planets\n",
      "   â€¢ False_Positive (2): FP, FA - False positives and false alarms\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ’¾ Exporting processed data...\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = 'tess_3class'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"ğŸ“‚ Output directory: {output_dir}/\")\n",
    "\n",
    "# Save train-test splits\n",
    "print(\"ğŸ’¾ Saving train-test splits...\")\n",
    "X_train_scaled.to_csv(f'{output_dir}/X_train_scaled.csv', index=False)\n",
    "X_test_scaled.to_csv(f'{output_dir}/X_test_scaled.csv', index=False)\n",
    "y_train.to_csv(f'{output_dir}/y_train.csv', index=False)\n",
    "y_test.to_csv(f'{output_dir}/y_test.csv', index=False)\n",
    "\n",
    "# Save full processed dataset\n",
    "print(\"ğŸ’¾ Saving full processed dataset...\")\n",
    "X_final.to_csv(f'{output_dir}/X_final_cleaned.csv', index=False)\n",
    "y_final.to_csv(f'{output_dir}/y_final_cleaned.csv', index=False)\n",
    "\n",
    "# Save scaler\n",
    "print(\"ğŸ’¾ Saving scaler...\")\n",
    "joblib.dump(scaler, f'{output_dir}/scaler.joblib')\n",
    "\n",
    "# Save target mapping\n",
    "print(\"ğŸ’¾ Saving target mapping...\")\n",
    "with open(f'{output_dir}/target_mapping.json', 'w') as f:\n",
    "    json.dump(target_mapping, f, indent=2)\n",
    "\n",
    "# Create metadata\n",
    "metadata = {\n",
    "    'dataset_name': 'TESS TOI 3-Class',\n",
    "    'original_shape': df.shape,\n",
    "    'final_samples': len(X_final),\n",
    "    'features': len(X_final.columns),\n",
    "    'target_classes': 3,\n",
    "    'class_distribution': y_final.value_counts().sort_index().to_dict(),\n",
    "    'train_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'feature_names': X_final.columns.tolist(),\n",
    "    'preprocessing_steps': [\n",
    "        'Target creation (3-class from TFOPWG_DISP)',\n",
    "        'Feature selection and engineering',\n",
    "        'Object column processing and encoding',\n",
    "        'Missing value imputation (>85% missing dropped)',\n",
    "        'Standard scaling',\n",
    "        'Train-test split (80/20)'\n",
    "    ],\n",
    "    'preprocessing_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'class_descriptions': target_mapping['class_descriptions'],\n",
    "    'original_dispositions': disposition_mapping\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "print(\"ğŸ’¾ Saving metadata...\")\n",
    "with open(f'{output_dir}/metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "# Summary of exported files\n",
    "print(f\"\\nâœ… PREPROCESSING COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"ğŸ“‚ All files saved to: {output_dir}/\")\n",
    "print(f\"\\nğŸ“‹ Files created:\")\n",
    "print(f\"   â€¢ X_train_scaled.csv - Scaled training features ({X_train_scaled.shape})\")\n",
    "print(f\"   â€¢ X_test_scaled.csv - Scaled test features ({X_test_scaled.shape})\")\n",
    "print(f\"   â€¢ y_train.csv - Training targets ({len(y_train)} samples)\")\n",
    "print(f\"   â€¢ y_test.csv - Test targets ({len(y_test)} samples)\")\n",
    "print(f\"   â€¢ X_final_cleaned.csv - Full feature matrix ({X_final.shape})\")\n",
    "print(f\"   â€¢ y_final_cleaned.csv - Full target vector ({len(y_final)} samples)\")\n",
    "print(f\"   â€¢ scaler.joblib - Fitted StandardScaler\")\n",
    "print(f\"   â€¢ target_mapping.json - Class mappings and descriptions\")\n",
    "print(f\"   â€¢ metadata.json - Complete preprocessing metadata\")\n",
    "\n",
    "print(f\"\\nğŸ¯ READY FOR MODEL TRAINING!\")\n",
    "print(f\"ğŸ“Š Dataset: {len(X_final)} samples, {len(X_final.columns)} features, 3 classes\")\n",
    "print(f\"ğŸ† Class distribution: {dict(y_final.value_counts().sort_index())}\")\n",
    "print(f\"âš–ï¸ Class imbalance ratio: {y_final.value_counts().max() / y_final.value_counts().min():.1f}:1\")\n",
    "\n",
    "# TESS-specific information\n",
    "print(f\"\\nğŸŒŸ TESS-SPECIFIC INFO:\")\n",
    "print(f\"ğŸ“¡ Source: TESS Objects of Interest (TOI) Catalog\")\n",
    "print(f\"ğŸ¯ 3-Class Mapping:\")\n",
    "print(f\"   â€¢ Candidate (0): PC, APC - Planet candidates needing follow-up\")\n",
    "print(f\"   â€¢ Confirmed (1): CP, KP - Confirmed and known planets\")\n",
    "print(f\"   â€¢ False_Positive (2): FP, FA - False positives and false alarms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
