{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6f8a22a",
   "metadata": {},
   "source": [
    "# Kepler Exoplanet Data Preprocessing Pipeline - 3-Class Classification\n",
    "\n",
    "This notebook implements a comprehensive preprocessing pipeline for the Kepler exoplanet dataset from NASA's Kepler mission for 3-class classification.\n",
    "\n",
    "## Dataset Information:\n",
    "- **Source**: Kepler Objects of Interest (KOI) Cumulative Table\n",
    "- **Size**: 9,564 objects Ã— 141 features\n",
    "- **Target Classes**: CONFIRMED (2,746), FALSE POSITIVE (4,839), CANDIDATE (1,979)\n",
    "\n",
    "## Preprocessing Steps:\n",
    "1. **Data Loading and Initial Exploration** - Load CSV, check dimensions, data types, and missing values\n",
    "2. **Target Variable Creation** - 3-class classification: Confirmed vs Candidate vs False Positive\n",
    "3. **Feature Selection and Engineering** - Process Kepler-specific features\n",
    "4. **Missing Value Handling** - Clean and impute missing data\n",
    "5. **Data Scaling and Splitting** - Prepare for model training\n",
    "6. **Export Processed Data** - Save for model training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a88d0fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Libraries imported successfully!\n",
      "ğŸš€ Ready for Kepler data preprocessing!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options for better output formatting\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"ğŸ“š Libraries imported successfully!\")\n",
    "print(\"ğŸš€ Ready for Kepler data preprocessing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a665f16d",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration\n",
    "\n",
    "Load the Kepler CSV file and perform initial data exploration including:\n",
    "- Dataset dimensions (rows/columns)\n",
    "- Data types examination\n",
    "- Missing value analysis\n",
    "- Target distribution analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a126d70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== KEPLER DATASET OVERVIEW ===\n",
      "ğŸ“Š Dataset shape: 9564 rows Ã— 141 columns\n",
      "ğŸ’¾ Memory usage: 19.73 MB\n",
      "\n",
      "=== TARGET DISTRIBUTION ===\n",
      "koi_disposition\n",
      "FALSE POSITIVE    4839\n",
      "CONFIRMED         2746\n",
      "CANDIDATE         1979\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ“ˆ Target percentages:\n",
      "   FALSE POSITIVE: 4839 (50.6%)\n",
      "   CONFIRMED: 2746 (28.7%)\n",
      "   CANDIDATE: 1979 (20.7%)\n",
      "\n",
      "=== DATA TYPES SUMMARY ===\n",
      "float64    117\n",
      "object      17\n",
      "int64        7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== MISSING VALUES ANALYSIS ===\n",
      "Total missing values: 237,112 (17.58%)\n",
      "\n",
      "ğŸ“‰ Top 10 columns with missing values:\n",
      "   koi_model_dof: 9564 (100.0%)\n",
      "   koi_teq_err1: 9564 (100.0%)\n",
      "   koi_ingress_err1: 9564 (100.0%)\n",
      "   koi_ingress: 9564 (100.0%)\n",
      "   koi_sage_err2: 9564 (100.0%)\n",
      "   koi_sage_err1: 9564 (100.0%)\n",
      "   koi_sma_err1: 9564 (100.0%)\n",
      "   koi_longp_err2: 9564 (100.0%)\n",
      "   koi_longp_err1: 9564 (100.0%)\n",
      "   koi_longp: 9564 (100.0%)\n",
      "\n",
      "=== SAMPLE DATA (First 3 rows) ===\n",
      "   rowid     kepid kepoi_name   kepler_name koi_disposition koi_vet_stat  \\\n",
      "0      1  10797460  K00752.01  Kepler-227 b       CONFIRMED         Done   \n",
      "1      2  10797460  K00752.02  Kepler-227 c       CONFIRMED         Done   \n",
      "2      3  10811496  K00753.01           NaN       CANDIDATE         Done   \n",
      "\n",
      "  koi_vet_date koi_pdisposition  koi_score  koi_fpflag_nt  koi_fpflag_ss  \\\n",
      "0   2018-08-16        CANDIDATE      1.000              0              0   \n",
      "1   2018-08-16        CANDIDATE      0.969              0              0   \n",
      "2   2018-08-16        CANDIDATE      0.000              0              0   \n",
      "\n",
      "   koi_fpflag_co  koi_fpflag_ec        koi_disp_prov    koi_comment  \\\n",
      "0              0              0  q1_q17_dr25_sup_koi     NO_COMMENT   \n",
      "1              0              0  q1_q17_dr25_sup_koi     NO_COMMENT   \n",
      "2              0              0  q1_q17_dr25_sup_koi  DEEP_V_SHAPED   \n",
      "\n",
      "   koi_period  koi_period_err1  koi_period_err2  koi_time0bk  \\\n",
      "0    9.488036         0.000028        -0.000028   170.538750   \n",
      "1   54.418383         0.000248        -0.000248   162.513840   \n",
      "2   19.899140         0.000015        -0.000015   175.850252   \n",
      "\n",
      "   koi_time0bk_err1  koi_time0bk_err2    koi_time0  koi_time0_err1  \\\n",
      "0          0.002160         -0.002160  2455003.539        0.002160   \n",
      "1          0.003520         -0.003520  2454995.514        0.003520   \n",
      "2          0.000581         -0.000581  2455008.850        0.000581   \n",
      "\n",
      "   koi_time0_err2  koi_eccen  koi_eccen_err1  koi_eccen_err2  koi_longp  \\\n",
      "0       -0.002160        0.0             NaN             NaN        NaN   \n",
      "1       -0.003520        0.0             NaN             NaN        NaN   \n",
      "2       -0.000581        0.0             NaN             NaN        NaN   \n",
      "\n",
      "   koi_longp_err1  koi_longp_err2  koi_impact  koi_impact_err1  \\\n",
      "0             NaN             NaN       0.146            0.318   \n",
      "1             NaN             NaN       0.586            0.059   \n",
      "2             NaN             NaN       0.969            5.126   \n",
      "\n",
      "   koi_impact_err2  koi_duration  koi_duration_err1  koi_duration_err2  \\\n",
      "0           -0.146        2.9575             0.0819            -0.0819   \n",
      "1           -0.443        4.5070             0.1160            -0.1160   \n",
      "2           -0.077        1.7822             0.0341            -0.0341   \n",
      "\n",
      "   koi_ingress  koi_ingress_err1  koi_ingress_err2  koi_depth  koi_depth_err1  \\\n",
      "0          NaN               NaN               NaN      615.8            19.5   \n",
      "1          NaN               NaN               NaN      874.8            35.5   \n",
      "2          NaN               NaN               NaN    10829.0           171.0   \n",
      "\n",
      "   koi_depth_err2   koi_ror  koi_ror_err1  koi_ror_err2  koi_srho  \\\n",
      "0           -19.5  0.022344      0.000832     -0.000528   3.20796   \n",
      "1           -35.5  0.027954      0.009078     -0.001347   3.02368   \n",
      "2          -171.0  0.154046      5.034292     -0.042179   7.29555   \n",
      "\n",
      "   koi_srho_err1  koi_srho_err2 koi_fittype  koi_prad  koi_prad_err1  \\\n",
      "0        0.33173       -1.09986     LS+MCMC      2.26           0.26   \n",
      "1        2.20489       -2.49638     LS+MCMC      2.83           0.32   \n",
      "2       35.03293       -2.75453     LS+MCMC     14.60           3.92   \n",
      "\n",
      "   koi_prad_err2  koi_sma  koi_sma_err1  koi_sma_err2  koi_incl  \\\n",
      "0          -0.15   0.0853           NaN           NaN     89.66   \n",
      "1          -0.19   0.2734           NaN           NaN     89.57   \n",
      "2          -1.31   0.1419           NaN           NaN     88.96   \n",
      "\n",
      "   koi_incl_err1  koi_incl_err2  koi_teq  koi_teq_err1  koi_teq_err2  \\\n",
      "0            NaN            NaN    793.0           NaN           NaN   \n",
      "1            NaN            NaN    443.0           NaN           NaN   \n",
      "2            NaN            NaN    638.0           NaN           NaN   \n",
      "\n",
      "   koi_insol  koi_insol_err1  koi_insol_err2  koi_dor  koi_dor_err1  \\\n",
      "0      93.59           29.45          -16.65    24.81           2.6   \n",
      "1       9.11            2.87           -1.62    77.90          28.4   \n",
      "2      39.30           31.04          -10.49    53.50          25.7   \n",
      "\n",
      "   koi_dor_err2                   koi_limbdark_mod  koi_ldm_coeff4  \\\n",
      "0          -2.6  Claret (2011 A&A 529 75) ATLAS LS             0.0   \n",
      "1         -28.4  Claret (2011 A&A 529 75) ATLAS LS             0.0   \n",
      "2         -25.7  Claret (2011 A&A 529 75) ATLAS LS             0.0   \n",
      "\n",
      "   koi_ldm_coeff3  koi_ldm_coeff2  koi_ldm_coeff1    koi_parm_prov  \\\n",
      "0             0.0          0.2291          0.4603  q1_q17_dr25_koi   \n",
      "1             0.0          0.2291          0.4603  q1_q17_dr25_koi   \n",
      "2             0.0          0.2711          0.3858  q1_q17_dr25_koi   \n",
      "\n",
      "   koi_max_sngle_ev  koi_max_mult_ev  koi_model_snr  koi_count  \\\n",
      "0          5.135849        28.470820           35.8          2   \n",
      "1          7.027669        20.109507           25.8          2   \n",
      "2         37.159767       187.449100           76.3          1   \n",
      "\n",
      "   koi_num_transits  koi_tce_plnt_num koi_tce_delivname  \\\n",
      "0             142.0               1.0   q1_q17_dr25_tce   \n",
      "1              25.0               2.0   q1_q17_dr25_tce   \n",
      "2              56.0               1.0   q1_q17_dr25_tce   \n",
      "\n",
      "                       koi_quarters  koi_bin_oedp_sig  \\\n",
      "0  11111111111111111000000000000000            0.6864   \n",
      "1  11111111111111111000000000000000            0.0023   \n",
      "2  11111101110111011000000000000000            0.6624   \n",
      "\n",
      "                        koi_trans_mod  koi_model_dof  koi_model_chisq  \\\n",
      "0  Mandel and Agol (2002 ApJ 580 171)            NaN              NaN   \n",
      "1  Mandel and Agol (2002 ApJ 580 171)            NaN              NaN   \n",
      "2  Mandel and Agol (2002 ApJ 580 171)            NaN              NaN   \n",
      "\n",
      "                                    koi_datalink_dvr  \\\n",
      "0  010/010797/010797460/dv/kplr010797460-20160209...   \n",
      "1  010/010797/010797460/dv/kplr010797460-20160209...   \n",
      "2  010/010811/010811496/dv/kplr010811496-20160209...   \n",
      "\n",
      "                                    koi_datalink_dvs  koi_steff  \\\n",
      "0  010/010797/010797460/dv/kplr010797460-001-2016...     5455.0   \n",
      "1  010/010797/010797460/dv/kplr010797460-002-2016...     5455.0   \n",
      "2  010/010811/010811496/dv/kplr010811496-001-2016...     5853.0   \n",
      "\n",
      "   koi_steff_err1  koi_steff_err2  koi_slogg  koi_slogg_err1  koi_slogg_err2  \\\n",
      "0            81.0           -81.0      4.467           0.064          -0.096   \n",
      "1            81.0           -81.0      4.467           0.064          -0.096   \n",
      "2           158.0          -176.0      4.544           0.044          -0.176   \n",
      "\n",
      "   koi_smet  koi_smet_err1  koi_smet_err2  koi_srad  koi_srad_err1  \\\n",
      "0      0.14           0.15          -0.15     0.927          0.105   \n",
      "1      0.14           0.15          -0.15     0.927          0.105   \n",
      "2     -0.18           0.30          -0.30     0.868          0.233   \n",
      "\n",
      "   koi_srad_err2  koi_smass  koi_smass_err1  koi_smass_err2  koi_sage  \\\n",
      "0         -0.061      0.919           0.052          -0.046       NaN   \n",
      "1         -0.061      0.919           0.052          -0.046       NaN   \n",
      "2         -0.078      0.961           0.110          -0.121       NaN   \n",
      "\n",
      "   koi_sage_err1  koi_sage_err2         koi_sparprov         ra        dec  \\\n",
      "0            NaN            NaN  q1_q17_dr25_stellar  291.93423  48.141651   \n",
      "1            NaN            NaN  q1_q17_dr25_stellar  291.93423  48.141651   \n",
      "2            NaN            NaN  q1_q17_dr25_stellar  297.00482  48.134129   \n",
      "\n",
      "   koi_kepmag  koi_gmag  koi_rmag  koi_imag  koi_zmag  koi_jmag  koi_hmag  \\\n",
      "0      15.347    15.890     15.27    15.114    15.006    14.082    13.751   \n",
      "1      15.347    15.890     15.27    15.114    15.006    14.082    13.751   \n",
      "2      15.436    15.943     15.39    15.220    15.166    14.254    13.900   \n",
      "\n",
      "   koi_kmag  koi_fwm_stat_sig  koi_fwm_sra  koi_fwm_sra_err  koi_fwm_sdec  \\\n",
      "0    13.648             0.002    19.462294         0.000014      48.14191   \n",
      "1    13.648             0.003    19.462265         0.000020      48.14199   \n",
      "2    13.826             0.278    19.800321         0.000002      48.13412   \n",
      "\n",
      "   koi_fwm_sdec_err  koi_fwm_srao  koi_fwm_srao_err  koi_fwm_sdeco  \\\n",
      "0           0.00013         0.430             0.510          0.940   \n",
      "1           0.00019        -0.630             0.720          1.230   \n",
      "2           0.00002        -0.021             0.069         -0.038   \n",
      "\n",
      "   koi_fwm_sdeco_err  koi_fwm_prao  koi_fwm_prao_err  koi_fwm_pdeco  \\\n",
      "0              0.480      -0.00020           0.00032       -0.00055   \n",
      "1              0.680       0.00066           0.00065       -0.00105   \n",
      "2              0.071       0.00070           0.00240        0.00060   \n",
      "\n",
      "   koi_fwm_pdeco_err  koi_dicco_mra  koi_dicco_mra_err  koi_dicco_mdec  \\\n",
      "0            0.00031         -0.010               0.13           0.200   \n",
      "1            0.00063          0.390               0.36           0.000   \n",
      "2            0.00340         -0.025               0.07          -0.034   \n",
      "\n",
      "   koi_dicco_mdec_err  koi_dicco_msky  koi_dicco_msky_err  koi_dikco_mra  \\\n",
      "0                0.16           0.200               0.170          0.080   \n",
      "1                0.48           0.390               0.360          0.490   \n",
      "2                0.07           0.042               0.072          0.002   \n",
      "\n",
      "   koi_dikco_mra_err  koi_dikco_mdec  koi_dikco_mdec_err  koi_dikco_msky  \\\n",
      "0              0.130           0.310               0.170           0.320   \n",
      "1              0.340           0.120               0.730           0.500   \n",
      "2              0.071          -0.027               0.074           0.027   \n",
      "\n",
      "   koi_dikco_msky_err  \n",
      "0               0.160  \n",
      "1               0.450  \n",
      "2               0.074  \n",
      "ğŸ’¾ Memory usage: 19.73 MB\n",
      "\n",
      "=== TARGET DISTRIBUTION ===\n",
      "koi_disposition\n",
      "FALSE POSITIVE    4839\n",
      "CONFIRMED         2746\n",
      "CANDIDATE         1979\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ“ˆ Target percentages:\n",
      "   FALSE POSITIVE: 4839 (50.6%)\n",
      "   CONFIRMED: 2746 (28.7%)\n",
      "   CANDIDATE: 1979 (20.7%)\n",
      "\n",
      "=== DATA TYPES SUMMARY ===\n",
      "float64    117\n",
      "object      17\n",
      "int64        7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== MISSING VALUES ANALYSIS ===\n",
      "Total missing values: 237,112 (17.58%)\n",
      "\n",
      "ğŸ“‰ Top 10 columns with missing values:\n",
      "   koi_model_dof: 9564 (100.0%)\n",
      "   koi_teq_err1: 9564 (100.0%)\n",
      "   koi_ingress_err1: 9564 (100.0%)\n",
      "   koi_ingress: 9564 (100.0%)\n",
      "   koi_sage_err2: 9564 (100.0%)\n",
      "   koi_sage_err1: 9564 (100.0%)\n",
      "   koi_sma_err1: 9564 (100.0%)\n",
      "   koi_longp_err2: 9564 (100.0%)\n",
      "   koi_longp_err1: 9564 (100.0%)\n",
      "   koi_longp: 9564 (100.0%)\n",
      "\n",
      "=== SAMPLE DATA (First 3 rows) ===\n",
      "   rowid     kepid kepoi_name   kepler_name koi_disposition koi_vet_stat  \\\n",
      "0      1  10797460  K00752.01  Kepler-227 b       CONFIRMED         Done   \n",
      "1      2  10797460  K00752.02  Kepler-227 c       CONFIRMED         Done   \n",
      "2      3  10811496  K00753.01           NaN       CANDIDATE         Done   \n",
      "\n",
      "  koi_vet_date koi_pdisposition  koi_score  koi_fpflag_nt  koi_fpflag_ss  \\\n",
      "0   2018-08-16        CANDIDATE      1.000              0              0   \n",
      "1   2018-08-16        CANDIDATE      0.969              0              0   \n",
      "2   2018-08-16        CANDIDATE      0.000              0              0   \n",
      "\n",
      "   koi_fpflag_co  koi_fpflag_ec        koi_disp_prov    koi_comment  \\\n",
      "0              0              0  q1_q17_dr25_sup_koi     NO_COMMENT   \n",
      "1              0              0  q1_q17_dr25_sup_koi     NO_COMMENT   \n",
      "2              0              0  q1_q17_dr25_sup_koi  DEEP_V_SHAPED   \n",
      "\n",
      "   koi_period  koi_period_err1  koi_period_err2  koi_time0bk  \\\n",
      "0    9.488036         0.000028        -0.000028   170.538750   \n",
      "1   54.418383         0.000248        -0.000248   162.513840   \n",
      "2   19.899140         0.000015        -0.000015   175.850252   \n",
      "\n",
      "   koi_time0bk_err1  koi_time0bk_err2    koi_time0  koi_time0_err1  \\\n",
      "0          0.002160         -0.002160  2455003.539        0.002160   \n",
      "1          0.003520         -0.003520  2454995.514        0.003520   \n",
      "2          0.000581         -0.000581  2455008.850        0.000581   \n",
      "\n",
      "   koi_time0_err2  koi_eccen  koi_eccen_err1  koi_eccen_err2  koi_longp  \\\n",
      "0       -0.002160        0.0             NaN             NaN        NaN   \n",
      "1       -0.003520        0.0             NaN             NaN        NaN   \n",
      "2       -0.000581        0.0             NaN             NaN        NaN   \n",
      "\n",
      "   koi_longp_err1  koi_longp_err2  koi_impact  koi_impact_err1  \\\n",
      "0             NaN             NaN       0.146            0.318   \n",
      "1             NaN             NaN       0.586            0.059   \n",
      "2             NaN             NaN       0.969            5.126   \n",
      "\n",
      "   koi_impact_err2  koi_duration  koi_duration_err1  koi_duration_err2  \\\n",
      "0           -0.146        2.9575             0.0819            -0.0819   \n",
      "1           -0.443        4.5070             0.1160            -0.1160   \n",
      "2           -0.077        1.7822             0.0341            -0.0341   \n",
      "\n",
      "   koi_ingress  koi_ingress_err1  koi_ingress_err2  koi_depth  koi_depth_err1  \\\n",
      "0          NaN               NaN               NaN      615.8            19.5   \n",
      "1          NaN               NaN               NaN      874.8            35.5   \n",
      "2          NaN               NaN               NaN    10829.0           171.0   \n",
      "\n",
      "   koi_depth_err2   koi_ror  koi_ror_err1  koi_ror_err2  koi_srho  \\\n",
      "0           -19.5  0.022344      0.000832     -0.000528   3.20796   \n",
      "1           -35.5  0.027954      0.009078     -0.001347   3.02368   \n",
      "2          -171.0  0.154046      5.034292     -0.042179   7.29555   \n",
      "\n",
      "   koi_srho_err1  koi_srho_err2 koi_fittype  koi_prad  koi_prad_err1  \\\n",
      "0        0.33173       -1.09986     LS+MCMC      2.26           0.26   \n",
      "1        2.20489       -2.49638     LS+MCMC      2.83           0.32   \n",
      "2       35.03293       -2.75453     LS+MCMC     14.60           3.92   \n",
      "\n",
      "   koi_prad_err2  koi_sma  koi_sma_err1  koi_sma_err2  koi_incl  \\\n",
      "0          -0.15   0.0853           NaN           NaN     89.66   \n",
      "1          -0.19   0.2734           NaN           NaN     89.57   \n",
      "2          -1.31   0.1419           NaN           NaN     88.96   \n",
      "\n",
      "   koi_incl_err1  koi_incl_err2  koi_teq  koi_teq_err1  koi_teq_err2  \\\n",
      "0            NaN            NaN    793.0           NaN           NaN   \n",
      "1            NaN            NaN    443.0           NaN           NaN   \n",
      "2            NaN            NaN    638.0           NaN           NaN   \n",
      "\n",
      "   koi_insol  koi_insol_err1  koi_insol_err2  koi_dor  koi_dor_err1  \\\n",
      "0      93.59           29.45          -16.65    24.81           2.6   \n",
      "1       9.11            2.87           -1.62    77.90          28.4   \n",
      "2      39.30           31.04          -10.49    53.50          25.7   \n",
      "\n",
      "   koi_dor_err2                   koi_limbdark_mod  koi_ldm_coeff4  \\\n",
      "0          -2.6  Claret (2011 A&A 529 75) ATLAS LS             0.0   \n",
      "1         -28.4  Claret (2011 A&A 529 75) ATLAS LS             0.0   \n",
      "2         -25.7  Claret (2011 A&A 529 75) ATLAS LS             0.0   \n",
      "\n",
      "   koi_ldm_coeff3  koi_ldm_coeff2  koi_ldm_coeff1    koi_parm_prov  \\\n",
      "0             0.0          0.2291          0.4603  q1_q17_dr25_koi   \n",
      "1             0.0          0.2291          0.4603  q1_q17_dr25_koi   \n",
      "2             0.0          0.2711          0.3858  q1_q17_dr25_koi   \n",
      "\n",
      "   koi_max_sngle_ev  koi_max_mult_ev  koi_model_snr  koi_count  \\\n",
      "0          5.135849        28.470820           35.8          2   \n",
      "1          7.027669        20.109507           25.8          2   \n",
      "2         37.159767       187.449100           76.3          1   \n",
      "\n",
      "   koi_num_transits  koi_tce_plnt_num koi_tce_delivname  \\\n",
      "0             142.0               1.0   q1_q17_dr25_tce   \n",
      "1              25.0               2.0   q1_q17_dr25_tce   \n",
      "2              56.0               1.0   q1_q17_dr25_tce   \n",
      "\n",
      "                       koi_quarters  koi_bin_oedp_sig  \\\n",
      "0  11111111111111111000000000000000            0.6864   \n",
      "1  11111111111111111000000000000000            0.0023   \n",
      "2  11111101110111011000000000000000            0.6624   \n",
      "\n",
      "                        koi_trans_mod  koi_model_dof  koi_model_chisq  \\\n",
      "0  Mandel and Agol (2002 ApJ 580 171)            NaN              NaN   \n",
      "1  Mandel and Agol (2002 ApJ 580 171)            NaN              NaN   \n",
      "2  Mandel and Agol (2002 ApJ 580 171)            NaN              NaN   \n",
      "\n",
      "                                    koi_datalink_dvr  \\\n",
      "0  010/010797/010797460/dv/kplr010797460-20160209...   \n",
      "1  010/010797/010797460/dv/kplr010797460-20160209...   \n",
      "2  010/010811/010811496/dv/kplr010811496-20160209...   \n",
      "\n",
      "                                    koi_datalink_dvs  koi_steff  \\\n",
      "0  010/010797/010797460/dv/kplr010797460-001-2016...     5455.0   \n",
      "1  010/010797/010797460/dv/kplr010797460-002-2016...     5455.0   \n",
      "2  010/010811/010811496/dv/kplr010811496-001-2016...     5853.0   \n",
      "\n",
      "   koi_steff_err1  koi_steff_err2  koi_slogg  koi_slogg_err1  koi_slogg_err2  \\\n",
      "0            81.0           -81.0      4.467           0.064          -0.096   \n",
      "1            81.0           -81.0      4.467           0.064          -0.096   \n",
      "2           158.0          -176.0      4.544           0.044          -0.176   \n",
      "\n",
      "   koi_smet  koi_smet_err1  koi_smet_err2  koi_srad  koi_srad_err1  \\\n",
      "0      0.14           0.15          -0.15     0.927          0.105   \n",
      "1      0.14           0.15          -0.15     0.927          0.105   \n",
      "2     -0.18           0.30          -0.30     0.868          0.233   \n",
      "\n",
      "   koi_srad_err2  koi_smass  koi_smass_err1  koi_smass_err2  koi_sage  \\\n",
      "0         -0.061      0.919           0.052          -0.046       NaN   \n",
      "1         -0.061      0.919           0.052          -0.046       NaN   \n",
      "2         -0.078      0.961           0.110          -0.121       NaN   \n",
      "\n",
      "   koi_sage_err1  koi_sage_err2         koi_sparprov         ra        dec  \\\n",
      "0            NaN            NaN  q1_q17_dr25_stellar  291.93423  48.141651   \n",
      "1            NaN            NaN  q1_q17_dr25_stellar  291.93423  48.141651   \n",
      "2            NaN            NaN  q1_q17_dr25_stellar  297.00482  48.134129   \n",
      "\n",
      "   koi_kepmag  koi_gmag  koi_rmag  koi_imag  koi_zmag  koi_jmag  koi_hmag  \\\n",
      "0      15.347    15.890     15.27    15.114    15.006    14.082    13.751   \n",
      "1      15.347    15.890     15.27    15.114    15.006    14.082    13.751   \n",
      "2      15.436    15.943     15.39    15.220    15.166    14.254    13.900   \n",
      "\n",
      "   koi_kmag  koi_fwm_stat_sig  koi_fwm_sra  koi_fwm_sra_err  koi_fwm_sdec  \\\n",
      "0    13.648             0.002    19.462294         0.000014      48.14191   \n",
      "1    13.648             0.003    19.462265         0.000020      48.14199   \n",
      "2    13.826             0.278    19.800321         0.000002      48.13412   \n",
      "\n",
      "   koi_fwm_sdec_err  koi_fwm_srao  koi_fwm_srao_err  koi_fwm_sdeco  \\\n",
      "0           0.00013         0.430             0.510          0.940   \n",
      "1           0.00019        -0.630             0.720          1.230   \n",
      "2           0.00002        -0.021             0.069         -0.038   \n",
      "\n",
      "   koi_fwm_sdeco_err  koi_fwm_prao  koi_fwm_prao_err  koi_fwm_pdeco  \\\n",
      "0              0.480      -0.00020           0.00032       -0.00055   \n",
      "1              0.680       0.00066           0.00065       -0.00105   \n",
      "2              0.071       0.00070           0.00240        0.00060   \n",
      "\n",
      "   koi_fwm_pdeco_err  koi_dicco_mra  koi_dicco_mra_err  koi_dicco_mdec  \\\n",
      "0            0.00031         -0.010               0.13           0.200   \n",
      "1            0.00063          0.390               0.36           0.000   \n",
      "2            0.00340         -0.025               0.07          -0.034   \n",
      "\n",
      "   koi_dicco_mdec_err  koi_dicco_msky  koi_dicco_msky_err  koi_dikco_mra  \\\n",
      "0                0.16           0.200               0.170          0.080   \n",
      "1                0.48           0.390               0.360          0.490   \n",
      "2                0.07           0.042               0.072          0.002   \n",
      "\n",
      "   koi_dikco_mra_err  koi_dikco_mdec  koi_dikco_mdec_err  koi_dikco_msky  \\\n",
      "0              0.130           0.310               0.170           0.320   \n",
      "1              0.340           0.120               0.730           0.500   \n",
      "2              0.071          -0.027               0.074           0.027   \n",
      "\n",
      "   koi_dikco_msky_err  \n",
      "0               0.160  \n",
      "1               0.450  \n",
      "2               0.074  \n"
     ]
    }
   ],
   "source": [
    "# Load the Kepler dataset\n",
    "df = pd.read_csv('kepler.csv')\n",
    "\n",
    "# Basic dataset information\n",
    "print(\"=== KEPLER DATASET OVERVIEW ===\")\n",
    "print(f\"ğŸ“Š Dataset shape: {df.shape[0]} rows Ã— {df.shape[1]} columns\")\n",
    "print(f\"ğŸ’¾ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Target variable analysis\n",
    "print(\"\\n=== TARGET DISTRIBUTION ===\")\n",
    "target_counts = df['koi_disposition'].value_counts()\n",
    "print(target_counts)\n",
    "print(f\"\\nğŸ“ˆ Target percentages:\")\n",
    "for disposition, count in target_counts.items():\n",
    "    pct = count / len(df) * 100\n",
    "    print(f\"   {disposition}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Data types overview\n",
    "print(\"\\n=== DATA TYPES SUMMARY ===\")\n",
    "dtype_counts = df.dtypes.value_counts()\n",
    "print(dtype_counts)\n",
    "\n",
    "# Missing values analysis\n",
    "print(\"\\n=== MISSING VALUES ANALYSIS ===\")\n",
    "missing_total = df.isnull().sum().sum()\n",
    "missing_pct = (missing_total / (df.shape[0] * df.shape[1])) * 100\n",
    "print(f\"Total missing values: {missing_total:,} ({missing_pct:.2f}%)\")\n",
    "\n",
    "# Columns with highest missing values\n",
    "missing_by_col = df.isnull().sum().sort_values(ascending=False)\n",
    "top_missing = missing_by_col[missing_by_col > 0].head(10)\n",
    "if not top_missing.empty:\n",
    "    print(\"\\nğŸ“‰ Top 10 columns with missing values:\")\n",
    "    for col, missing_count in top_missing.items():\n",
    "        missing_pct_col = (missing_count / len(df)) * 100\n",
    "        print(f\"   {col}: {missing_count} ({missing_pct_col:.1f}%)\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\n=== SAMPLE DATA (First 3 rows) ===\")\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9696c801",
   "metadata": {},
   "source": [
    "## 2. Target Variable Creation - 3-Class Classification\n",
    "\n",
    "Convert the Kepler disposition categories into a 3-class classification problem:\n",
    "- **Candidate (0)**: Objects marked as CANDIDATE\n",
    "- **Confirmed (1)**: Objects marked as CONFIRMED  \n",
    "- **False_Positive (2)**: Objects marked as FALSE POSITIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6ec70ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Creating 3-class target variable...\n",
      "\n",
      "ğŸ“Š FINAL 3-CLASS TARGET DISTRIBUTION:\n",
      "   0 (Candidate): 1979 (20.7%)\n",
      "   1 (Confirmed): 2746 (28.7%)\n",
      "   2 (False_Positive): 4839 (50.6%)\n",
      "\n",
      "âš–ï¸ Class imbalance ratio: 2.4:1\n",
      "\n",
      "âœ… Target variable created successfully!\n",
      "ğŸ“ˆ Final dataset shape: (9564, 142)\n"
     ]
    }
   ],
   "source": [
    "# Create 3-class target variable\n",
    "print(\"ğŸ¯ Creating 3-class target variable...\")\n",
    "\n",
    "# Map Kepler dispositions to 3 classes\n",
    "disposition_mapping = {\n",
    "    'CANDIDATE': 0,        # Planet candidates needing follow-up\n",
    "    'CONFIRMED': 1,        # Confirmed exoplanets\n",
    "    'FALSE POSITIVE': 2    # False positives and refuted objects\n",
    "}\n",
    "\n",
    "# Create target variable\n",
    "df['target_3class'] = df['koi_disposition'].map(disposition_mapping)\n",
    "\n",
    "# Check for any unmapped values\n",
    "unmapped = df[df['target_3class'].isnull()]['koi_disposition'].unique()\n",
    "if len(unmapped) > 0:\n",
    "    print(f\"âš ï¸ Unmapped dispositions found: {unmapped}\")\n",
    "    # Handle any edge cases here if needed\n",
    "    \n",
    "# Remove rows with missing target\n",
    "initial_count = len(df)\n",
    "df = df.dropna(subset=['target_3class'])\n",
    "removed_count = initial_count - len(df)\n",
    "if removed_count > 0:\n",
    "    print(f\"ğŸ—‘ï¸ Removed {removed_count} rows with missing target\")\n",
    "\n",
    "# Final target distribution\n",
    "print(\"\\nğŸ“Š FINAL 3-CLASS TARGET DISTRIBUTION:\")\n",
    "target_counts = df['target_3class'].value_counts().sort_index()\n",
    "class_names = ['Candidate', 'Confirmed', 'False_Positive']\n",
    "\n",
    "for class_id, count in target_counts.items():\n",
    "    class_name = class_names[int(class_id)]\n",
    "    pct = count / len(df) * 100\n",
    "    print(f\"   {class_id} ({class_name}): {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Class imbalance analysis\n",
    "class_counts = target_counts.values\n",
    "imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "print(f\"\\nâš–ï¸ Class imbalance ratio: {imbalance_ratio:.1f}:1\")\n",
    "\n",
    "# Create target mapping for reference\n",
    "target_mapping = {\n",
    "    'original_mapping': disposition_mapping,\n",
    "    'encoding': {str(k): v for v, k in enumerate(class_names)},\n",
    "    'class_descriptions': {\n",
    "        'Candidate': 'Planet candidates requiring follow-up observation',\n",
    "        'Confirmed': 'Confirmed exoplanets with high confidence',\n",
    "        'False_Positive': 'False positives and refuted planetary candidates'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nâœ… Target variable created successfully!\")\n",
    "print(f\"ğŸ“ˆ Final dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628af8ae",
   "metadata": {},
   "source": [
    "## 3. Feature Selection and Engineering\n",
    "\n",
    "Select and engineer relevant features from the Kepler dataset:\n",
    "- Remove non-predictive columns (IDs, names, comments)\n",
    "- Process numerical features\n",
    "- Handle error columns and flags\n",
    "- Create derived features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df852f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Starting feature selection and engineering...\n",
      "ğŸ“‹ Total columns in dataset: 142\n",
      "ğŸ¯ Feature columns selected: 126\n",
      "\n",
      "ğŸ—‘ï¸ Excluded columns (16):\n",
      "   â€¢ rowid\n",
      "   â€¢ kepid\n",
      "   â€¢ kepoi_name\n",
      "   â€¢ kepler_name\n",
      "   â€¢ koi_disposition\n",
      "   â€¢ koi_pdisposition\n",
      "   â€¢ target_3class\n",
      "   â€¢ koi_comment\n",
      "   â€¢ koi_disp_prov\n",
      "   â€¢ koi_parm_prov\n",
      "   â€¢ koi_sparprov\n",
      "   â€¢ koi_tce_delivname\n",
      "   â€¢ koi_datalink_dvr\n",
      "   â€¢ koi_datalink_dvs\n",
      "   â€¢ koi_limbdark_mod\n",
      "   â€¢ koi_vet_date\n",
      "\n",
      "ğŸ“Š FEATURE ANALYSIS:\n",
      "   Numerical features: 122\n",
      "   Object features: 4\n",
      "\n",
      "ğŸ”¤ Object columns requiring attention:\n",
      "   â€¢ koi_vet_stat: 1 unique values\n",
      "     Sample: ['Done']\n",
      "   â€¢ koi_fittype: 4 unique values\n",
      "     Sample: ['LS+MCMC' 'MCMC' 'LS' 'none']\n",
      "   â€¢ koi_quarters: 212 unique values\n",
      "   â€¢ koi_trans_mod: 1 unique values\n",
      "     Sample: ['Mandel and Agol (2002 ApJ 580 171)']\n",
      "\n",
      "âœ… Feature selection completed!\n",
      "ğŸ“ˆ Features shape: (9564, 126)\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”§ Starting feature selection and engineering...\")\n",
    "\n",
    "# Columns to exclude from features (non-predictive)\n",
    "exclude_columns = [\n",
    "    # ID and name columns\n",
    "    'rowid', 'kepid', 'kepoi_name', 'kepler_name',\n",
    "    \n",
    "    # Target and disposition columns\n",
    "    'koi_disposition', 'koi_pdisposition', 'target_3class',\n",
    "    \n",
    "    # Comments and metadata\n",
    "    'koi_comment', 'koi_disp_prov', 'koi_parm_prov', 'koi_sparprov',\n",
    "    \n",
    "    # Data links and deliverables\n",
    "    'koi_tce_delivname', 'koi_datalink_dvr', 'koi_datalink_dvs',\n",
    "    \n",
    "    # Transit model specific (keep koi_trans_mod, exclude detailed limb darkening)\n",
    "    'koi_limbdark_mod',\n",
    "    \n",
    "    # Date columns (keep if needed for temporal analysis)\n",
    "    'koi_vet_date'\n",
    "]\n",
    "\n",
    "# Get all column names\n",
    "all_columns = df.columns.tolist()\n",
    "print(f\"ğŸ“‹ Total columns in dataset: {len(all_columns)}\")\n",
    "\n",
    "# Feature columns (excluding target and non-predictive)\n",
    "feature_columns = [col for col in all_columns if col not in exclude_columns]\n",
    "print(f\"ğŸ¯ Feature columns selected: {len(feature_columns)}\")\n",
    "\n",
    "# Display excluded columns\n",
    "print(f\"\\nğŸ—‘ï¸ Excluded columns ({len(exclude_columns)}):\")\n",
    "for col in exclude_columns:\n",
    "    if col in all_columns:\n",
    "        print(f\"   â€¢ {col}\")\n",
    "\n",
    "# Analyze feature types\n",
    "X_features = df[feature_columns].copy()\n",
    "\n",
    "print(f\"\\nğŸ“Š FEATURE ANALYSIS:\")\n",
    "print(f\"   Numerical features: {X_features.select_dtypes(include=[np.number]).shape[1]}\")\n",
    "print(f\"   Object features: {X_features.select_dtypes(include=['object']).shape[1]}\")\n",
    "\n",
    "# Check for object/string columns that might need processing\n",
    "object_cols = X_features.select_dtypes(include=['object']).columns.tolist()\n",
    "if object_cols:\n",
    "    print(f\"\\nğŸ”¤ Object columns requiring attention:\")\n",
    "    for col in object_cols:\n",
    "        unique_values = X_features[col].nunique()\n",
    "        print(f\"   â€¢ {col}: {unique_values} unique values\")\n",
    "        if unique_values <= 10:  # Show sample values for categorical\n",
    "            sample_values = X_features[col].dropna().unique()[:5]\n",
    "            print(f\"     Sample: {sample_values}\")\n",
    "\n",
    "print(f\"\\nâœ… Feature selection completed!\")\n",
    "print(f\"ğŸ“ˆ Features shape: {X_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3f3b41",
   "metadata": {},
   "source": [
    "## 4. Handle Object/Categorical Columns\n",
    "\n",
    "Process categorical and object columns in the Kepler dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c23884c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¤ Processing object/categorical columns...\n",
      "ğŸ“… Processing koi_quarters...\n",
      "ğŸ”¬ Processing koi_fittype...\n",
      "   Fit types: {'LS+MCMC': 7897, 'MCMC': 1206, 'none': 369, 'LS': 92}\n",
      "   Encoding: {'LS+MCMC': 0, 'MCMC': 1, 'none': 2, 'LS': 3}\n",
      "ğŸŒŒ Processing koi_trans_mod...\n",
      "   Transit models: {'Mandel and Agol (2002 ApJ 580 171)': 9201}\n",
      "   Most common model: Mandel and Agol (2002 ApJ 580 171)\n",
      "ğŸ—‘ï¸ Dropped original object columns: ['koi_fittype', 'koi_trans_mod', 'koi_quarters']\n",
      "âš ï¸ Remaining object columns: ['koi_vet_stat']\n",
      "ğŸ—‘ï¸ Dropped remaining object columns\n",
      "\n",
      "âœ… Object column processing completed!\n",
      "ğŸ“ˆ Processed features shape: (9564, 125)\n",
      "ğŸ”¢ All columns are now numerical: True\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”¤ Processing object/categorical columns...\")\n",
    "\n",
    "# Handle specific object columns in Kepler dataset\n",
    "X_processed = X_features.copy()\n",
    "\n",
    "# Process koi_quarters (quarters when observed)\n",
    "if 'koi_quarters' in X_processed.columns:\n",
    "    print(\"ğŸ“… Processing koi_quarters...\")\n",
    "    # Convert quarters to number of quarters observed\n",
    "    X_processed['quarters_count'] = X_processed['koi_quarters'].apply(\n",
    "        lambda x: str(x).count('1') if pd.notna(x) else 0\n",
    "    )\n",
    "    # Keep original for now, will decide later\n",
    "    \n",
    "# Process koi_fittype (fitting method)\n",
    "if 'koi_fittype' in X_processed.columns:\n",
    "    print(\"ğŸ”¬ Processing koi_fittype...\")\n",
    "    fittype_counts = X_processed['koi_fittype'].value_counts()\n",
    "    print(f\"   Fit types: {fittype_counts.to_dict()}\")\n",
    "    \n",
    "    # Convert to numerical encoding\n",
    "    fittype_mapping = {}\n",
    "    for i, fittype in enumerate(fittype_counts.index):\n",
    "        if pd.notna(fittype):\n",
    "            fittype_mapping[fittype] = i\n",
    "    \n",
    "    X_processed['fittype_encoded'] = X_processed['koi_fittype'].map(fittype_mapping)\n",
    "    print(f\"   Encoding: {fittype_mapping}\")\n",
    "\n",
    "# Process koi_trans_mod (transit model)\n",
    "if 'koi_trans_mod' in X_processed.columns:\n",
    "    print(\"ğŸŒŒ Processing koi_trans_mod...\")\n",
    "    transmod_counts = X_processed['koi_trans_mod'].value_counts()\n",
    "    print(f\"   Transit models: {transmod_counts.to_dict()}\")\n",
    "    \n",
    "    # Most common transit model gets 1, others get 0\n",
    "    most_common_model = transmod_counts.index[0] if len(transmod_counts) > 0 else None\n",
    "    X_processed['is_mandel_agol'] = (X_processed['koi_trans_mod'] == most_common_model).astype(int)\n",
    "    print(f\"   Most common model: {most_common_model}\")\n",
    "\n",
    "# Drop original object columns after processing\n",
    "object_cols_to_drop = ['koi_fittype', 'koi_trans_mod', 'koi_quarters']\n",
    "existing_to_drop = [col for col in object_cols_to_drop if col in X_processed.columns]\n",
    "if existing_to_drop:\n",
    "    X_processed = X_processed.drop(columns=existing_to_drop)\n",
    "    print(f\"ğŸ—‘ï¸ Dropped original object columns: {existing_to_drop}\")\n",
    "\n",
    "# Check remaining object columns\n",
    "remaining_objects = X_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "if remaining_objects:\n",
    "    print(f\"âš ï¸ Remaining object columns: {remaining_objects}\")\n",
    "    # Drop them for now\n",
    "    X_processed = X_processed.drop(columns=remaining_objects)\n",
    "    print(f\"ğŸ—‘ï¸ Dropped remaining object columns\")\n",
    "\n",
    "print(f\"\\nâœ… Object column processing completed!\")\n",
    "print(f\"ğŸ“ˆ Processed features shape: {X_processed.shape}\")\n",
    "print(f\"ğŸ”¢ All columns are now numerical: {X_processed.select_dtypes(include=[np.number]).shape[1] == X_processed.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3d609c",
   "metadata": {},
   "source": [
    "## 5. Missing Value Analysis and Handling\n",
    "\n",
    "Analyze and handle missing values in the processed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d56a7e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ•³ï¸ Analyzing and handling missing values...\n",
      "ğŸ“Š Columns with missing values: 111 out of 125\n",
      "\n",
      "ğŸ“‰ Top 15 columns with missing values:\n",
      "   koi_ingress_err1: 9564 (100.0%)\n",
      "   koi_model_chisq: 9564 (100.0%)\n",
      "   koi_longp_err1: 9564 (100.0%)\n",
      "   koi_longp: 9564 (100.0%)\n",
      "   koi_eccen_err2: 9564 (100.0%)\n",
      "   koi_eccen_err1: 9564 (100.0%)\n",
      "   koi_model_dof: 9564 (100.0%)\n",
      "   koi_ingress: 9564 (100.0%)\n",
      "   koi_ingress_err2: 9564 (100.0%)\n",
      "   koi_sage: 9564 (100.0%)\n",
      "   koi_sage_err1: 9564 (100.0%)\n",
      "   koi_sma_err1: 9564 (100.0%)\n",
      "   koi_sma_err2: 9564 (100.0%)\n",
      "   koi_sage_err2: 9564 (100.0%)\n",
      "   koi_incl_err1: 9564 (100.0%)\n",
      "\n",
      "ğŸ› ï¸ Missing value handling strategy:\n",
      "ğŸ—‘ï¸ Dropping 19 columns with >80% missing values\n",
      "   â€¢ koi_ingress_err1 (100.0% missing)\n",
      "   â€¢ koi_model_chisq (100.0% missing)\n",
      "   â€¢ koi_longp_err1 (100.0% missing)\n",
      "   â€¢ koi_longp (100.0% missing)\n",
      "   â€¢ koi_eccen_err2 (100.0% missing)\n",
      "   ... and 14 more\n",
      "\n",
      "ğŸ”§ Imputing 44,101 remaining missing values...\n",
      "âœ… Missing values after imputation: 0\n",
      "\n",
      "ğŸ“ˆ Final processed features shape: (9564, 106)\n",
      "ğŸ¯ Ready for model training!\n",
      "âœ… Missing values after imputation: 0\n",
      "\n",
      "ğŸ“ˆ Final processed features shape: (9564, 106)\n",
      "ğŸ¯ Ready for model training!\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ•³ï¸ Analyzing and handling missing values...\")\n",
    "\n",
    "# Missing value analysis\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'column': X_processed.columns,\n",
    "    'missing_count': X_processed.isnull().sum(),\n",
    "    'missing_pct': (X_processed.isnull().sum() / len(X_processed)) * 100\n",
    "}).sort_values('missing_pct', ascending=False)\n",
    "\n",
    "# Filter columns with missing values\n",
    "missing_cols = missing_analysis[missing_analysis['missing_count'] > 0]\n",
    "\n",
    "print(f\"ğŸ“Š Columns with missing values: {len(missing_cols)} out of {len(X_processed.columns)}\")\n",
    "\n",
    "if len(missing_cols) > 0:\n",
    "    print(\"\\nğŸ“‰ Top 15 columns with missing values:\")\n",
    "    for _, row in missing_cols.head(15).iterrows():\n",
    "        print(f\"   {row['column']}: {row['missing_count']} ({row['missing_pct']:.1f}%)\")\n",
    "    \n",
    "    # Strategy for handling missing values\n",
    "    print(\"\\nğŸ› ï¸ Missing value handling strategy:\")\n",
    "    \n",
    "    # 1. Drop columns with >80% missing values\n",
    "    high_missing_cols = missing_cols[missing_cols['missing_pct'] > 80]['column'].tolist()\n",
    "    if high_missing_cols:\n",
    "        print(f\"ğŸ—‘ï¸ Dropping {len(high_missing_cols)} columns with >80% missing values\")\n",
    "        X_processed = X_processed.drop(columns=high_missing_cols)\n",
    "        for col in high_missing_cols[:5]:  # Show first 5\n",
    "            missing_pct = missing_cols[missing_cols['column'] == col]['missing_pct'].iloc[0]\n",
    "            print(f\"   â€¢ {col} ({missing_pct:.1f}% missing)\")\n",
    "        if len(high_missing_cols) > 5:\n",
    "            print(f\"   ... and {len(high_missing_cols) - 5} more\")\n",
    "    \n",
    "    # 2. Impute remaining missing values\n",
    "    remaining_missing = X_processed.isnull().sum().sum()\n",
    "    if remaining_missing > 0:\n",
    "        print(f\"\\nğŸ”§ Imputing {remaining_missing:,} remaining missing values...\")\n",
    "        \n",
    "        # Use median imputation for numerical features\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        X_processed_imputed = pd.DataFrame(\n",
    "            imputer.fit_transform(X_processed),\n",
    "            columns=X_processed.columns,\n",
    "            index=X_processed.index\n",
    "        )\n",
    "        \n",
    "        # Verify no missing values remain\n",
    "        final_missing = X_processed_imputed.isnull().sum().sum()\n",
    "        print(f\"âœ… Missing values after imputation: {final_missing}\")\n",
    "        \n",
    "        X_processed = X_processed_imputed\n",
    "    \n",
    "else:\n",
    "    print(\"âœ… No missing values found!\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Final processed features shape: {X_processed.shape}\")\n",
    "print(f\"ğŸ¯ Ready for model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427062b8",
   "metadata": {},
   "source": [
    "## 6. Data Scaling and Train-Test Split\n",
    "\n",
    "Split the data and apply scaling for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "329cf227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš–ï¸ Splitting data and applying scaling...\n",
      "ğŸ“Š Final dataset shape: X=(9564, 106), y=(9564,)\n",
      "âœ… X and y are properly aligned\n",
      "\n",
      "ğŸ“Š TRAIN-TEST SPLIT RESULTS:\n",
      "   Training set: 7651 samples (80.0%)\n",
      "   Test set: 1913 samples (20.0%)\n",
      "\n",
      "ğŸ“ˆ Class distribution in splits:\n",
      "   Train:\n",
      "     0 (Candidate): 1583 (20.7%)\n",
      "     1 (Confirmed): 2197 (28.7%)\n",
      "     2 (False_Positive): 3871 (50.6%)\n",
      "   Test:\n",
      "     0 (Candidate): 396 (20.7%)\n",
      "     1 (Confirmed): 549 (28.7%)\n",
      "     2 (False_Positive): 968 (50.6%)\n",
      "\n",
      "ğŸ”§ Applying StandardScaler...\n",
      "âœ… Scaling completed!\n",
      "ğŸ“Š Scaled features - Train: (7651, 106), Test: (1913, 106)\n",
      "\n",
      "ğŸ“ Scaling verification (training set):\n",
      "   Mean: 0.000000 (should be ~0)\n",
      "   Std: 0.971762 (should be ~1)\n"
     ]
    }
   ],
   "source": [
    "print(\"âš–ï¸ Splitting data and applying scaling...\")\n",
    "\n",
    "# Prepare final dataset\n",
    "X_final = X_processed.copy()\n",
    "y_final = df['target_3class'].copy()\n",
    "\n",
    "print(f\"ğŸ“Š Final dataset shape: X={X_final.shape}, y={y_final.shape}\")\n",
    "\n",
    "# Verify alignment\n",
    "assert len(X_final) == len(y_final), \"X and y must have same number of samples\"\n",
    "print(\"âœ… X and y are properly aligned\")\n",
    "\n",
    "# Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y_final,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_final\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š TRAIN-TEST SPLIT RESULTS:\")\n",
    "print(f\"   Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X_final)*100:.1f}%)\")\n",
    "print(f\"   Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X_final)*100:.1f}%)\")\n",
    "\n",
    "# Check class distribution in splits\n",
    "print(f\"\\nğŸ“ˆ Class distribution in splits:\")\n",
    "class_names = ['Candidate', 'Confirmed', 'False_Positive']\n",
    "\n",
    "for split_name, y_split in [('Train', y_train), ('Test', y_test)]:\n",
    "    print(f\"   {split_name}:\")\n",
    "    split_counts = y_split.value_counts().sort_index()\n",
    "    for class_id, count in split_counts.items():\n",
    "        class_name = class_names[int(class_id)]\n",
    "        pct = count / len(y_split) * 100\n",
    "        print(f\"     {class_id} ({class_name}): {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Apply scaling\n",
    "print(f\"\\nğŸ”§ Applying StandardScaler...\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data and transform both sets\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "print(f\"âœ… Scaling completed!\")\n",
    "print(f\"ğŸ“Š Scaled features - Train: {X_train_scaled.shape}, Test: {X_test_scaled.shape}\")\n",
    "\n",
    "# Verify scaling\n",
    "print(f\"\\nğŸ“ Scaling verification (training set):\")\n",
    "print(f\"   Mean: {X_train_scaled.mean().mean():.6f} (should be ~0)\")\n",
    "print(f\"   Std: {X_train_scaled.std().mean():.6f} (should be ~1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69185c3",
   "metadata": {},
   "source": [
    "## 7. Export Processed Data\n",
    "\n",
    "Save the preprocessed data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb3b9a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Exporting processed data...\n",
      "ğŸ“‚ Output directory: kepler_3class/\n",
      "ğŸ’¾ Saving train-test splits...\n",
      "ğŸ’¾ Saving full processed dataset...\n",
      "ğŸ’¾ Saving full processed dataset...\n",
      "ğŸ’¾ Saving scaler...\n",
      "ğŸ’¾ Saving target mapping...\n",
      "ğŸ’¾ Saving metadata...\n",
      "\n",
      "âœ… PREPROCESSING COMPLETED SUCCESSFULLY!\n",
      "ğŸ“‚ All files saved to: kepler_3class/\n",
      "\n",
      "ğŸ“‹ Files created:\n",
      "   â€¢ X_train_scaled.csv - Scaled training features ((7651, 106))\n",
      "   â€¢ X_test_scaled.csv - Scaled test features ((1913, 106))\n",
      "   â€¢ y_train.csv - Training targets (7651 samples)\n",
      "   â€¢ y_test.csv - Test targets (1913 samples)\n",
      "   â€¢ X_final_cleaned.csv - Full feature matrix ((9564, 106))\n",
      "   â€¢ y_final_cleaned.csv - Full target vector (9564 samples)\n",
      "   â€¢ scaler.joblib - Fitted StandardScaler\n",
      "   â€¢ target_mapping.json - Class mappings and descriptions\n",
      "   â€¢ metadata.json - Complete preprocessing metadata\n",
      "\n",
      "ğŸ¯ READY FOR MODEL TRAINING!\n",
      "ğŸ“Š Dataset: 9564 samples, 106 features, 3 classes\n",
      "ğŸ† Class distribution: {0: 1979, 1: 2746, 2: 4839}\n",
      "âš–ï¸ Class imbalance ratio: 2.4:1\n",
      "ğŸ’¾ Saving scaler...\n",
      "ğŸ’¾ Saving target mapping...\n",
      "ğŸ’¾ Saving metadata...\n",
      "\n",
      "âœ… PREPROCESSING COMPLETED SUCCESSFULLY!\n",
      "ğŸ“‚ All files saved to: kepler_3class/\n",
      "\n",
      "ğŸ“‹ Files created:\n",
      "   â€¢ X_train_scaled.csv - Scaled training features ((7651, 106))\n",
      "   â€¢ X_test_scaled.csv - Scaled test features ((1913, 106))\n",
      "   â€¢ y_train.csv - Training targets (7651 samples)\n",
      "   â€¢ y_test.csv - Test targets (1913 samples)\n",
      "   â€¢ X_final_cleaned.csv - Full feature matrix ((9564, 106))\n",
      "   â€¢ y_final_cleaned.csv - Full target vector (9564 samples)\n",
      "   â€¢ scaler.joblib - Fitted StandardScaler\n",
      "   â€¢ target_mapping.json - Class mappings and descriptions\n",
      "   â€¢ metadata.json - Complete preprocessing metadata\n",
      "\n",
      "ğŸ¯ READY FOR MODEL TRAINING!\n",
      "ğŸ“Š Dataset: 9564 samples, 106 features, 3 classes\n",
      "ğŸ† Class distribution: {0: 1979, 1: 2746, 2: 4839}\n",
      "âš–ï¸ Class imbalance ratio: 2.4:1\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ’¾ Exporting processed data...\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = 'kepler_3class'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"ğŸ“‚ Output directory: {output_dir}/\")\n",
    "\n",
    "# Save train-test splits\n",
    "print(\"ğŸ’¾ Saving train-test splits...\")\n",
    "X_train_scaled.to_csv(f'{output_dir}/X_train_scaled.csv', index=False)\n",
    "X_test_scaled.to_csv(f'{output_dir}/X_test_scaled.csv', index=False)\n",
    "y_train.to_csv(f'{output_dir}/y_train.csv', index=False)\n",
    "y_test.to_csv(f'{output_dir}/y_test.csv', index=False)\n",
    "\n",
    "# Save full processed dataset\n",
    "print(\"ğŸ’¾ Saving full processed dataset...\")\n",
    "X_final.to_csv(f'{output_dir}/X_final_cleaned.csv', index=False)\n",
    "y_final.to_csv(f'{output_dir}/y_final_cleaned.csv', index=False)\n",
    "\n",
    "# Save scaler\n",
    "print(\"ğŸ’¾ Saving scaler...\")\n",
    "joblib.dump(scaler, f'{output_dir}/scaler.joblib')\n",
    "\n",
    "# Save target mapping\n",
    "print(\"ğŸ’¾ Saving target mapping...\")\n",
    "with open(f'{output_dir}/target_mapping.json', 'w') as f:\n",
    "    json.dump(target_mapping, f, indent=2)\n",
    "\n",
    "# Create metadata\n",
    "metadata = {\n",
    "    'dataset_name': 'Kepler Exoplanet 3-Class',\n",
    "    'original_shape': df.shape,\n",
    "    'final_samples': len(X_final),\n",
    "    'features': len(X_final.columns),\n",
    "    'target_classes': 3,\n",
    "    'class_distribution': y_final.value_counts().sort_index().to_dict(),\n",
    "    'train_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'feature_names': X_final.columns.tolist(),\n",
    "    'preprocessing_steps': [\n",
    "        'Target creation (3-class)',\n",
    "        'Feature selection and engineering',\n",
    "        'Object column processing',\n",
    "        'Missing value imputation',\n",
    "        'Standard scaling',\n",
    "        'Train-test split (80/20)'\n",
    "    ],\n",
    "    'preprocessing_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'class_descriptions': target_mapping['class_descriptions']\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "print(\"ğŸ’¾ Saving metadata...\")\n",
    "with open(f'{output_dir}/metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "# Summary of exported files\n",
    "print(f\"\\nâœ… PREPROCESSING COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"ğŸ“‚ All files saved to: {output_dir}/\")\n",
    "print(f\"\\nğŸ“‹ Files created:\")\n",
    "print(f\"   â€¢ X_train_scaled.csv - Scaled training features ({X_train_scaled.shape})\")\n",
    "print(f\"   â€¢ X_test_scaled.csv - Scaled test features ({X_test_scaled.shape})\")\n",
    "print(f\"   â€¢ y_train.csv - Training targets ({len(y_train)} samples)\")\n",
    "print(f\"   â€¢ y_test.csv - Test targets ({len(y_test)} samples)\")\n",
    "print(f\"   â€¢ X_final_cleaned.csv - Full feature matrix ({X_final.shape})\")\n",
    "print(f\"   â€¢ y_final_cleaned.csv - Full target vector ({len(y_final)} samples)\")\n",
    "print(f\"   â€¢ scaler.joblib - Fitted StandardScaler\")\n",
    "print(f\"   â€¢ target_mapping.json - Class mappings and descriptions\")\n",
    "print(f\"   â€¢ metadata.json - Complete preprocessing metadata\")\n",
    "\n",
    "print(f\"\\nğŸ¯ READY FOR MODEL TRAINING!\")\n",
    "print(f\"ğŸ“Š Dataset: {len(X_final)} samples, {len(X_final.columns)} features, 3 classes\")\n",
    "print(f\"ğŸ† Class distribution: {dict(y_final.value_counts().sort_index())}\")\n",
    "print(f\"âš–ï¸ Class imbalance ratio: {y_final.value_counts().max() / y_final.value_counts().min():.1f}:1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
